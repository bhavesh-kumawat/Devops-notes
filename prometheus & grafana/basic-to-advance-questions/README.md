ðŸŸ¢ Prometheus
1. What is Prometheus and why is it used?
Prometheus is an open-source monitoring and alerting system designed for metrics-based monitoring of applications and infrastructure.
It was originally built by SoundCloud and is now part of the Cloud Native Computing Foundation (CNCF).
In simple terms:

ðŸ‘‰ Prometheus collects metrics, stores them, lets you query them, and triggers alerts.
ðŸ”¹ Why is Prometheus used?
Prometheus is used to:
â€¢ Monitor applications and servers
â€¢ Track performance and availability
â€¢ Detect issues before outages
â€¢ Generate alerts when thresholds are crossed
â€¢ Monitor dynamic cloud and Kubernetes environments
ðŸ”¹ Why Prometheus is Popular in DevOps
â€¢ Cloud-native & Kubernetes-friendly
â€¢ Handles ephemeral containers well
â€¢ No dependency on external storage
â€¢ Open-source & vendor-neutral
â€¢ Strong ecosystem (Grafana, Alertmanager)
2. Push vs Pull monitoring â€” why does Prometheus use pull?
ðŸ”¹ Push vs Pull Monitoring (Quick Recap)
Push model
â€¢ Targets send (push) metrics to the monitoring system
â€¢ Example: CloudWatch, older monitoring tools
â€¢ Agent decides when and what to send
Pull model
â€¢ Monitoring system scrapes (pulls) metrics from targets
â€¢ Example: Prometheus
â€¢ Prometheus controls when and how often metrics are collected
ðŸ”¹ Why Prometheus Uses the Pull Model
1ï¸âƒ£ Better Reliability & Control
Prometheus decides:
â€¢ Scrape interval
â€¢ Timeout
â€¢ Target availability
If a target doesnâ€™t respond â†’ Prometheus knows itâ€™s down

ðŸ‘‰ With push, silence could mean network issue or app crash (hard to tell).
2ï¸âƒ£ Service Discovery (Perfect for Kubernetes)
In dynamic environments:
â€¢ Pods are created & destroyed frequently
â€¢ IPs change constantly
Prometheus:
â€¢ Automatically discovers targets
â€¢ Starts scraping new services
â€¢ Stops scraping dead ones
ðŸ”¥ This is huge for Kubernetes & microservices.
3ï¸âƒ£ No Metrics Loss During Spikes
If Prometheus is slow or down:
â€¢ Pull model = targets donâ€™t need to buffer metrics
â€¢ Prometheus resumes scraping later
With push:
â€¢ Targets may drop metrics
â€¢ Or overload the monitoring system
4ï¸âƒ£ Simpler Security Model
â€¢ Only Prometheus needs network access
â€¢ Targets expose /metrics endpoint
â€¢ Easier firewall rules
With push:
â€¢ Every service needs credentials to push metrics
â€¢ Higher security risk
5ï¸âƒ£ Consistent & Predictable Data
â€¢ Same scrape interval for all targets
â€¢ Easier comparisons across services
â€¢ Clean, uniform time-series data
6ï¸âƒ£ Built-in Health Checking
If Prometheus canâ€™t scrape:
â€¢ Target is unhealthy or unreachable
â€¢ This itself becomes a metric (up == 0)
ðŸ’¡ Monitoring system monitors availability by design.
ðŸ”¹ Butâ€¦ When Is Push Useful?
Prometheus still supports push via Pushgateway, for:
â€¢ Short-lived jobs (batch jobs, cron jobs)
â€¢ Jobs that finish before scrape happens
âš ï¸ Pushgateway is an exception, not the default.
3. What are metrics? Name the metric types in Prometheus.
Metrics are numerical measurements collected over time that describe the state or performance of a system.
In monitoring, metrics answer questions like:
â€¢ How many requests are coming in?
â€¢ How fast is the application responding?
â€¢ How much CPU or memory is being used?
â€¢ Is the system healthy or failing?
In Prometheus, metrics are stored as time-series data, meaning:metric name + labels + timestamp + value
ðŸ”¹ Metric Types in Prometheus
Prometheus supports four main metric types:
1ï¸âƒ£ Counter
What it is:
â€¢ A value that only increases (or resets to zero on restart)
Used for:
â€¢ Counting events
Examples:
â€¢ HTTP requests
â€¢ Errors
â€¢ Jobs completed
Example metric:

http_requests_total

Important interview note:
â€¢ Counters are commonly used with rate() or increase() in PromQL
2ï¸âƒ£ Gauge
What it is:
â€¢ A value that can go up and down
Used for:
â€¢ Current state measurements
Examples:
â€¢ CPU usage
â€¢ Memory usage
â€¢ Number of active connections
â€¢ Queue length
Example metric:

node_memory_available_bytes

3ï¸âƒ£ Histogram
What it is:
â€¢ Samples observations and counts them in buckets
Used for:
â€¢ Request latency
â€¢ Response size
Provides:
â€¢ Bucket counts
â€¢ Sum of observations
â€¢ Total count
Example metrics:

http_request_duration_seconds_bucket
http_request_duration_seconds_sum
http_request_duration_seconds_count

4ï¸âƒ£ Summary
What it is:
â€¢ Similar to histogram, but calculates quantiles (e.g. 95th percentile)
Used for:
â€¢ Latency percentiles
Key difference from histogram:
â€¢ Quantiles are calculated on the client side
â€¢ Not aggregatable across instances
Example metric:

http_request_duration_seconds{quantile="0.95"}

4. What is an exporter?
An exporter is a service or agent that collects metrics from a system and exposes them in Prometheus format, usually over an HTTP endpoint like /metrics.
ðŸ‘‰ Prometheus scrapes exporters to collect metrics.
ðŸ”¹ Why Are Exporters Needed?
Many systems:
â€¢ Donâ€™t natively expose Prometheus metrics
â€¢ Use proprietary or legacy formats
Exporters act as a bridge between those systems and Prometheus.
ðŸ”¹ How Exporters Work (Simple Flow)

System â†’ Exporter â†’ /metrics endpoint â†’ Prometheus

Example:

Linux Server â†’ Node Exporter â†’ Prometheus

ðŸ”¹ Common Prometheus Exporters
Here are some you should name in interviews:
ðŸ”¹ Node Exporter
â€¢ Exposes OS-level metrics
â€¢ CPU, memory, disk, network
ðŸ”¹ Blackbox Exporter
â€¢ Probes endpoints (HTTP, TCP, ICMP)
â€¢ Checks availability & latency
ðŸ”¹ cAdvisor
â€¢ Container-level metrics
ðŸ”¹ MySQL / PostgreSQL Exporter
â€¢ Database metrics
ðŸ”¹ JVM Exporter
â€¢ Java application metrics
ðŸ”¹ NGINX / Apache Exporter
â€¢ Web server metrics
5. What is a scrape interval?
A scrape interval is the frequency at which Prometheus pulls (scrapes) metrics from a target (application or exporter).
ðŸ‘‰ In short:
How often Prometheus collects metrics.
ðŸ”¹ Example
If:

scrape_interval: 15s

Prometheus will collect metrics every 15 seconds from the target.
ðŸ”¹ Where Is It Defined?
â€¢ Globally (applies to all targets)
â€¢ Per job (overrides global value)
Example:

global:
  scrape_interval: 15s

scrape_configs:
  - job_name: "node"
    scrape_interval: 30s

ðŸ”¹ Why Scrape Interval Matters
Choosing the right interval is a trade-off:
Short interval (e.g., 5s)
âœ… Faster alerting

âœ… More granular data

âŒ Higher CPU, memory, and storage usage
Long interval (e.g., 60s)
âœ… Lower resource usage

âŒ Slower detection of issues

âŒ Less detailed graphs
6. Difference between Prometheus and traditional monitoring tools (Nagios, Zabbix)?
FeaturePrometheusNagios / ZabbixMonitoring typeMetrics-basedCheck-basedData modelTime-seriesStatus checksArchitecturePull-basedMostly push-basedInfrastructureDynamicStaticCloud/K8s supportNativeLimited / complexService discoveryBuilt-inManualAlertingPromQL-basedThreshold-basedVisualizationGrafanaBuilt-in UIScalingHorizontalHarderUse caseMicroservices, K8sServers, legacy systems
7. Explain PromQL. What is it used for?
PromQL (Prometheus Query Language) is the query language used by Prometheus to retrieve, filter, aggregate, and analyze time-series metrics stored in Prometheus.
ðŸ‘‰ In short:
PromQL is used to turn raw metrics into meaningful insights.
ðŸ”¹ What is PromQL Used For?
PromQL is used to:
â€¢ Query metrics from Prometheus
â€¢ Aggregate and calculate rates
â€¢ Build Grafana dashboards
â€¢ Define alerting rules
â€¢ Perform troubleshooting & root-cause analysis
8. Difference between rate() and irate()?
Both functions calculate the per-second rate of increase of a counter metric.
The difference is how much data they use.
ðŸ”¹ rate()
âœ… What it does
â€¢ Calculates the average rate of increase over a time range
â€¢ Uses multiple data points in the range
ðŸ“Œ Syntax

rate(http_requests_total[5m])

ðŸ‘ Best for
â€¢ Dashboards
â€¢ Alerting
â€¢ Stable trends
â€¢ SLO/SLA calculations
ðŸ’¡ Behavior
â€¢ Smooths out spikes
â€¢ Less noisy
â€¢ More reliable
ðŸ”¹ irate()
âš¡ What it does
â€¢ Calculates the instant rate
â€¢ Uses only the last two data points
ðŸ“Œ Syntax

irate(http_requests_total[5m])

ðŸ‘ Best for
â€¢ Debugging
â€¢ Spotting sudden spikes
â€¢ Short-term analysis
âš ï¸ Behavior
â€¢ Very sensitive to spikes
â€¢ Can be noisy
â€¢ Not recommended for alerts
9. What is service discovery in Prometheus?
Service discovery in Prometheus is the mechanism by which Prometheus automatically finds targets (services, pods, nodes) to scrape metrics from without manual configuration.
ðŸ‘‰ Instead of hard-coding IPs and ports, Prometheus dynamically discovers them.
ðŸ”¹ Why Service Discovery Is Needed
Modern environments are dynamic:
â€¢ Pods are created and destroyed
â€¢ IPs change frequently
â€¢ Auto-scaling adds/removes instances
Manual target configuration does not scale.
ðŸ”¹ How Service Discovery Works
1. Prometheus queries a service discovery source
2. Gets a list of targets + metadata (labels)
3. Applies relabeling rules
4. Starts scraping valid targets
5. Stops scraping removed targets
ðŸ”¹ Types of Service Discovery in Prometheus
You should name at least 3â€“4 in interviews:
ðŸ”¹ Static Config
â€¢ Manually defined targets
â€¢ Used in small or static setups

static_configs:
  - targets: ["10.0.0.1:9100"]

ðŸ”¹ Kubernetes Service Discovery (Most Important)
â€¢ Discovers:
    â—¦ Pods
    â—¦ Services
    â—¦ Nodes
    â—¦ Endpoints

kubernetes_sd_configs:
  - role: pod

Labels from Kubernetes become Prometheus labels.
ðŸ”¹ Cloud Providers
â€¢ AWS EC2
â€¢ GCP
â€¢ Azure
Prometheus discovers instances via cloud APIs.
ðŸ”¹ File-Based Discovery
â€¢ Targets defined in files
â€¢ Files can be updated dynamically
10. How does Prometheus store data?
  Prometheus stores data as time-series data in a local time-series database (TSDB) on disk.
Each data point is stored as:

metric name + labels + timestamp + value

Example:

http_requests_total{method="GET", status="200"} @ 1700000000 â†’ 15432

ðŸ”¹ What Is a Time Series?
A time series is a stream of values over time for a unique combination of metric name and labels.
ðŸ‘‰ Each unique label set = one time series.
ðŸ”¹ Prometheus Storage Architecture (High Level)
Prometheus stores data in blocks on local disk:
1ï¸âƒ£ In-Memory (Head Block)
â€¢ Recent samples are stored in memory
â€¢ Fast reads & writes
â€¢ Periodically flushed to disk
2ï¸âƒ£ Persistent Storage (TSDB Blocks)
â€¢ Data written in immutable blocks
â€¢ Each block typically covers 2 hours
â€¢ Blocks contain:
    â—¦ Samples
    â—¦ Index
    â—¦ Metadata
3ï¸âƒ£ Write-Ahead Log (WAL)
â€¢ Every incoming sample is written to WAL
â€¢ Ensures crash recovery
â€¢ On restart, WAL is replayed
ðŸ”¹ Retention Policy
Prometheus stores data for a limited time:
Example:

--storage.tsdb.retention.time=15d

Older data is automatically deleted.
11. What is Alertmanager?
Alertmanager is a component of the Prometheus ecosystem that manages alerts generated by Prometheus.
ðŸ‘‰ Prometheus detects problems.

ðŸ‘‰ Alertmanager handles and delivers those alerts.
ðŸ”¹ What Does Alertmanager Do?
Alertmanager is responsible for:
â€¢ Deduplication â€“ avoids duplicate alerts
â€¢ Grouping â€“ combines related alerts
â€¢ Routing â€“ sends alerts to the right team
â€¢ Inhibition â€“ suppresses lower-priority alerts
â€¢ Notification delivery â€“ email, Slack, PagerDuty, etc.
ðŸ”¹ How Alertmanager Works (Flow)

Metrics â†’ Prometheus â†’ Alert rules â†’ Alertmanager â†’ Notifications

1. Prometheus evaluates alert rules
2. Fires alerts when conditions are met
3. Sends alerts to Alertmanager
4. Alertmanager processes & routes them
5. Notifications are sent
12. How do you monitor Kubernetes with Prometheus?
We monitor Kubernetes with Prometheus by deploying Prometheus inside the cluster, using exporters and Kubernetes service discovery to collect metrics from nodes, pods, containers, and cluster components, and then visualizing and alerting on those metrics.
ðŸ”¹ Architecture (Interview-Friendly Flow)

Kubernetes Cluster
â”‚
â”œâ”€ Node Exporter â†’ Node metrics
â”œâ”€ kube-state-metrics â†’ K8s object state
â”œâ”€ cAdvisor â†’ Container metrics
â”œâ”€ App metrics (/metrics)
â”‚
â””â”€ Prometheus â†’ Alertmanager â†’ Grafana

13. How do you handle high cardinality issues?
High cardinality occurs when a metric has too many unique label combinations, creating a large number of time series.
Example of bad labels:
â€¢ user_id
â€¢ session_id
â€¢ request_id
â€¢ timestamp
Each unique value = new time series â†’ memory & CPU explosion.
ðŸ”¹ Why High Cardinality Is a Problem
â€¢ High memory usage
â€¢ Slow queries
â€¢ Increased disk usage
â€¢ Prometheus instability or crashes
ðŸ’¡ Prometheus performance depends heavily on number of time series, not just metrics count.
ðŸ”¹ How Do You Handle High Cardinality Issues?
1ï¸âƒ£ Avoid High-Cardinality Labels (Best Fix)
Do not use:
â€¢ User IDs
â€¢ Request IDs
â€¢ IP addresses (sometimes)
â€¢ Dynamic URLs
Instead:
â€¢ Use aggregated labels
â€¢ Bucket values (e.g. status code ranges)
2ï¸âƒ£ Use Metrics, Not Logs
â€¢ Metrics = aggregated data
â€¢ Logs = detailed per-request data
ðŸ‘‰ Use Loki / ELK for request-level details.
3ï¸âƒ£ Relabeling to Drop Labels
Drop unnecessary labels before ingestion:

metric_relabel_configs:
  - regex: "request_id|user_id"
    action: labeldrop

4ï¸âƒ£ Drop Unneeded Metrics
Reduce ingestion:

metric_relabel_configs:
  - source_labels: [__name__]
    regex: "http_request_duration_seconds_bucket"
    action: drop

5ï¸âƒ£ Control Histogram Buckets
Too many buckets = more series.
â€¢ Reduce bucket count
â€¢ Use reasonable bucket ranges
6ï¸âƒ£ Limit Scraped Targets
â€¢ Avoid scraping unnecessary pods
â€¢ Use annotations or ServiceMonitor selectors
â€¢ Filter targets with relabeling
7ï¸âƒ£ Use Recording Rules
Precompute heavy queries:

record: job:http_requests:rate5m
expr: sum(rate(http_requests_total[5m])) by (job)

Benefits:
â€¢ Faster queries
â€¢ Lower CPU usage
8ï¸âƒ£ Monitor Cardinality Itself (Interview Gold)
Use Prometheus metrics:

count({__name__=~".+"}) by (__name__)

or:

prometheus_tsdb_head_series

14. How does Prometheus scale?
Prometheus scales vertically by default, and for large or distributed systems it scales horizontally using federation or external systems like Thanos, Cortex, or Mimir.
ðŸ”¹ Why Scaling Is Non-Trivial in Prometheus
â€¢ Prometheus is single-node per instance
â€¢ Uses local TSDB
â€¢ No built-in clustering
ðŸ‘‰ This design keeps Prometheus simple and reliable, but requires architectural scaling.
ðŸ”¹ Scaling Methods in Prometheus
1ï¸âƒ£ Vertical Scaling (Default)
Increase:
â€¢ CPU
â€¢ Memory
â€¢ Disk
Used when:
â€¢ Small to medium environments
â€¢ Single cluster monitoring
âš ï¸ Limited by hardware.
2ï¸âƒ£ Horizontal Scaling by Sharding
Split scrape targets across multiple Prometheus servers:
â€¢ Prometheus A â†’ half the services
â€¢ Prometheus B â†’ other half
Benefits:
â€¢ Reduces load per instance
â€¢ Independent failures
3ï¸âƒ£ Federation (Hierarchical Scaling)
Lower-level Prometheus servers scrape metrics and a central Prometheus scrapes aggregated metrics from them.
Used for:
â€¢ Multi-cluster setups
â€¢ Global dashboards
Example:

Cluster Prometheus â†’ Global Prometheus

4ï¸âƒ£ Remote Storage / Long-Term Storage (Production Standard)
Use systems built on top of Prometheus:
ðŸ”¹ Thanos
â€¢ Global querying
â€¢ Long-term storage (S3, GCS)
â€¢ High availability
ðŸ”¹ Cortex / Mimir
â€¢ Horizontally scalable Prometheus backend
â€¢ Multi-tenant
â€¢ Used at very large scale
ðŸ’¡ Interview tip:â€œPrometheus handles collection; Thanos handles scale.â€
5ï¸âƒ£ High Availability (HA)
Run two Prometheus instances scraping the same targets:
â€¢ One active
â€¢ One standby
Alertmanager deduplicates alerts.
ðŸ”¹ How Prometheus Scales in Kubernetes
â€¢ Multiple Prometheus instances per cluster
â€¢ Prometheus Operator manages lifecycle
â€¢ Sharding via shards config
â€¢ External storage via Thanos
15. What is federation? When would you use it?
Federation is a Prometheus feature where one Prometheus server scrapes metrics from other Prometheus servers instead of scraping exporters directly.
ðŸ‘‰ In short:
Prometheus scraping Prometheus.
ðŸ”¹ How Federation Works
â€¢ Child Prometheus instances:
    â—¦ Scrape exporters in their local environment
â€¢ Parent (Federation) Prometheus:
    â—¦ Scrapes selected metrics from child Prometheus servers via /federate
Example flow:

Exporters â†’ Child Prometheus â†’ Parent Prometheus

Only aggregated or important metrics are federated.
ðŸ”¹ Example Use Case
â€¢ Each Kubernetes cluster has its own Prometheus
â€¢ A central Prometheus federates metrics from all clusters
â€¢ Used for global dashboards & alerts
ðŸ”¹ Why Use Federation?
1ï¸âƒ£ Scalability
â€¢ Reduces load on a single Prometheus
â€¢ Each instance handles a subset of targets
2ï¸âƒ£ Multi-Cluster / Multi-Region Monitoring
â€¢ Local Prometheus per cluster
â€¢ Central Prometheus for high-level visibility
3ï¸âƒ£ Isolation & Reliability
â€¢ If one cluster fails, others still work
â€¢ Prometheus instances fail independently
4ï¸âƒ£ Reduced Cardinality at Top Level
â€¢ Only aggregated metrics are federated
â€¢ Prevents high cardinality explosion in central Prometheus
ðŸ”¹ When Would You Use Federation?
Use federation when:
â€¢ You have multiple clusters or regions
â€¢ You need global dashboards
â€¢ You want hierarchical monitoring
â€¢ You donâ€™t need raw per-pod metrics centrally
ðŸ”¹ When NOT to Use Federation
â€¢ If you need long-term storage
â€¢ If you need raw metrics across clusters
â€¢ If you need global querying with deduplication
ðŸ‘‰ Use Thanos / Cortex / Mimir instead.
16. Explain Thanos / Cortex / Mimir.
ðŸ”¹ Why Thanos / Cortex / Mimir Exist (Context)
Prometheus by itself:
â€¢ Is single-node
â€¢ Stores data locally
â€¢ Has limited retention
â€¢ Has no global query
ðŸ‘‰ Thanos, Cortex, and Mimir extend Prometheus to solve these problems at scale.
ðŸ”¹ Thanos
ðŸ”¸ What is Thanos?
Thanos is a high-availability and long-term storage layer built on top of Prometheus.
It keeps Prometheus unchanged and adds global visibility.
ðŸ”¸ Key Features
â€¢ Global querying across Prometheus instances
â€¢ Long-term storage (S3, GCS, Azure Blob)
â€¢ High availability with deduplication
â€¢ Downsampling for cheaper storage
ðŸ”¸ How Thanos Works
â€¢ Prometheus runs normally
â€¢ Thanos Sidecar ships blocks to object storage
â€¢ Thanos Querier reads from:
    â—¦ Prometheus (real-time)
    â—¦ Object storage (historical)
ðŸ”¸ When to Use Thanos
âœ… You already use Prometheus

âœ… Multi-cluster / multi-region

âœ… Need HA & long-term storage

âœ… Want minimal changes
ðŸ”¹ Cortex
ðŸ”¸ What is Cortex?
Cortex is a horizontally scalable Prometheus backend.
Instead of local storage, metrics are:
â€¢ Pushed via remote_write
â€¢ Stored in object storage
â€¢ Queried centrally
ðŸ”¸ Key Features
â€¢ Massive horizontal scalability
â€¢ Multi-tenancy
â€¢ Long-term storage
â€¢ Cloud-native
ðŸ”¸ When to Use Cortex
âœ… SaaS platforms

âœ… Very large scale (millions of series)

âœ… Strong multi-tenancy requirements
âš ï¸ More complex to operate.
ðŸ”¹ Mimir
ðŸ”¸ What is Mimir?
Grafana Mimir is the successor to Cortex, designed to be:
â€¢ Simpler
â€¢ Faster
â€¢ More scalable
Used by Grafana Cloud.
ðŸ”¸ Key Features
â€¢ All Cortex features
â€¢ Better performance
â€¢ Easier operations
â€¢ Active development
ðŸ”¸ When to Use Mimir
âœ… New large-scale deployments

âœ… Grafana ecosystem users

âœ… Cloud-native monitoring at scale
17. How do you make Prometheus highly available?
ðŸ”¹ Why Prometheus Needs HA
â€¢ Prometheus is single-node by default
â€¢ If it crashes or a node goes down â†’ metrics collection stops
â€¢ Alerts may be missed
ðŸ’¡ HA ensures continuous monitoring and alerting, even if a Prometheus instance fails.
ðŸ”¹ Ways to Make Prometheus Highly Available
1ï¸âƒ£ Run Multiple Prometheus Instances
â€¢ Deploy two or more Prometheus servers scraping the same targets
â€¢ Each instance runs independently
â€¢ Use Alertmanager to deduplicate alerts
Example:

Prometheus-A â†’ scrapes all targets
Prometheus-B â†’ scrapes same targets
Alertmanager â†’ deduplicates alerts

âœ… Simple and effective

âš ï¸ Memory usage doubles (each instance stores its own data)
2ï¸âƒ£ Use Alertmanager for HA
â€¢ Alertmanager should also be redundant
â€¢ Multiple Alertmanager instances:
    â—¦ Clustered
    â—¦ Deduplicate alerts
    â—¦ Failover automatically
3ï¸âƒ£ Use Remote Storage for HA
â€¢ Prometheus writes to Thanos, Cortex, or Mimir
â€¢ Benefits:
    â—¦ Long-term storage
    â—¦ Query from multiple replicas
    â—¦ Failover if one Prometheus crashes
Thanos example:
â€¢ Prometheus Sidecar â†’ object storage
â€¢ Thanos Querier â†’ global queries
â€¢ Multiple Prometheus instances â†’ deduplication
4ï¸âƒ£ Sharding Targets
â€¢ For large environments:
    â—¦ Split scrape targets across Prometheus instances
    â—¦ Reduces load per instance
    â—¦ Combined with remote storage for full picture
5ï¸âƒ£ Kubernetes HA Deployment (Optional)
â€¢ Prometheus Operator supports replicas
â€¢ StatefulSets + PersistentVolumeClaims
â€¢ Works with Thanos Sidecar for HA
ðŸ”¹ HA Best Practices
â€¢ Always deduplicate alerts in Alertmanager
â€¢ Use remote storage for backup & global querying
â€¢ Monitor Prometheus itself
â€¢ Avoid single point of failure (storage, network)
18. How do you secure Prometheus endpoints?
ðŸ”¹ Why Securing Prometheus Endpoints Is Important
â€¢ Prometheus exposes metrics via HTTP /metrics endpoints
â€¢ Contains sensitive info (system metrics, app internals)
â€¢ Default Prometheus has no authentication
â€¢ Open endpoints can lead to:
    â—¦ Information leakage
    â—¦ Unauthorized access
    â—¦ Security breaches
ðŸ”¹ Key Strategies to Secure Prometheus
1ï¸âƒ£ Use Network-Level Security
â€¢ Restrict access with firewalls or security groups
â€¢ Only allow access from:
    â—¦ Monitoring servers
    â—¦ Trusted admin IPs
â€¢ In Kubernetes:
    â—¦ Use NetworkPolicies
    â—¦ Limit pod access
2ï¸âƒ£ Enable Authentication & Authorization
â€¢ Prometheus itself doesnâ€™t support auth natively
â€¢ Use a reverse proxy (Nginx, Envoy, Traefik) in front
    â—¦ Basic Auth or OAuth2
    â—¦ Role-based access control (RBAC)
Example with Nginx:

location / {
    auth_basic "Prometheus Login";
    auth_basic_user_file /etc/nginx/.htpasswd;
    proxy_pass http://prometheus:9090;
}

3ï¸âƒ£ Use TLS/HTTPS
â€¢ Encrypt traffic between Prometheus and clients
â€¢ Especially important for:
    â—¦ Remote Prometheus queries
    â—¦ Federated Prometheus setups
â€¢ Terminate TLS at reverse proxy or load balancer
4ï¸âƒ£ Protect /metrics Endpoints of Targets
â€¢ Node Exporter, cAdvisor, kube-state-metrics, etc. also expose /metrics
â€¢ Options:
    â—¦ Reverse proxy + auth
    â—¦ Network isolation
    â—¦ Use service accounts / RBAC in Kubernetes
5ï¸âƒ£ Limit Remote Write Access
â€¢ Remote write endpoints should be authenticated
â€¢ Use API keys or TLS mutual auth if supported
6ï¸âƒ£ Kubernetes Best Practices
â€¢ Run Prometheus in dedicated namespace
â€¢ Use NetworkPolicy to allow only scraping pods
â€¢ Avoid exposing Prometheus publicly unless needed
7ï¸âƒ£ Monitor & Audit
â€¢ Log all access to Prometheus endpoints
â€¢ Watch for unusual queries or scraping attempts
19. Explain histogram vs summary â€” when would you choose one over the other?
FeatureHistogramSummaryPurposeObserves and counts values in bucketsCalculates quantiles (percentiles) directlyData storedCount per bucket + sum + total countQuantiles (e.g., 0.5, 0.95) + sum + countAggregationâœ… Can aggregate across instancesâŒ Cannot aggregate quantiles across instancesUse caseLatency, request size, other distributionsLatency per instance when aggregation isnâ€™t neededScraping overheadHigher (many series for buckets)Lower (fewer series)
ðŸŸ¢ Grafana
20. What is Grafana?
Grafana is an open-source visualization and analytics platform for monitoring and observability.
It allows you to:
â€¢ Query data from various sources (Prometheus, Loki, Elasticsearch, InfluxDB, MySQL, etc.)
â€¢ Visualize metrics in dashboards (graphs, charts, heatmaps, tables)
â€¢ Set up alerts based on metric thresholds
ðŸ‘‰ In short: Prometheus collects the data, Grafana makes it human-readable and actionable.
ðŸ”¹ Key Features of Grafana
1. Multiple Data Sources
    â—¦ Prometheus, Loki, Elasticsearch, PostgreSQL, InfluxDB, etc.
2. Dashboards & Panels
    â—¦ Interactive visualizations: graphs, tables, heatmaps, bar charts
3. Alerting
    â—¦ Alerts based on thresholds or PromQL queries
    â—¦ Integrated with Slack, Email, PagerDuty
4. Annotations
    â—¦ Mark events (deployments, incidents) on dashboards for context
5. Templating & Variables
    â—¦ Dynamic dashboards (filter by service, cluster, or node)
6. Plugins & Extensibility
    â—¦ Community plugins for new panels, data sources, and apps
ðŸ”¹ How Grafana Works with Prometheus
â€¢ Prometheus exposes metrics via /metrics
â€¢ Grafana connects to Prometheus as a data source
â€¢ Users create dashboards with PromQL queries to visualize metrics
â€¢ Alerts can be configured in Grafana using the same queries
21. Difference between Grafana and Prometheus?
FeaturePrometheusGrafanaPurposeMonitoring & alertingVisualization & analyticsFunctionCollects, stores, and queries metricsVisualizes metrics and dashboardsData SourceMetrics from exporters or instrumented appsConnects to Prometheus, Loki, Elasticsearch, InfluxDB, etc.AlertingBuilt-in alerting via rules â†’ AlertmanagerAlerts based on dashboard queriesStorageLocal TSDB or remote storage (Thanos/Cortex)No native metric storageQuery LanguagePromQLUses PromQL (for Prometheus) or SQL (for other sources)VisualizationMinimal (basic graphs in Prometheus UI)Rich, interactive dashboards and panels
22. What are dashboards and panels?
Dashboards are collections of panels that provide a single view of your metrics and observability data.
â€¢ Think of a dashboard as a monitoring page for a system, service, or cluster.
â€¢ Dashboards can include multiple panels, variables, and filters.
â€¢ They help you visualize, track trends, and detect anomalies in one place.
Examples:
â€¢ Kubernetes cluster health dashboard
â€¢ Application latency dashboard
â€¢ Node or server resource usage dashboard
ðŸ”¹ What Are Panels?
Panels are the individual visualization units inside a dashboard.
â€¢ Each panel displays one query or a set of queries.
â€¢ Panels can take many forms:
    â—¦ Graphs (time-series)
    â—¦ Tables
    â—¦ Singlestat (single numeric value)
    â—¦ Heatmaps, pie charts, bar charts
Examples:
â€¢ CPU usage graph for a node
â€¢ HTTP request rate per service
â€¢ Error count table per microservice
23. What data sources does Grafana support?
A data source in Grafana is the system from which Grafana queries metrics, logs, or traces to display in panels.
â€¢ Grafana connects to many types of systems.
â€¢ Each data source has its own query language and features.
ðŸ”¹ Key Data Sources Grafana Supports
1ï¸âƒ£ Prometheus
â€¢ Query language: PromQL
â€¢ Metrics-focused
â€¢ Example use: application latency, request rate, CPU/memory metrics
2ï¸âƒ£ Loki
â€¢ Log aggregation system
â€¢ Query language: LogQL
â€¢ For logs visualization and correlation with metrics
3ï¸âƒ£ Elasticsearch
â€¢ Query language: Lucene / Elasticsearch Query DSL
â€¢ Logs or time-series data
â€¢ Useful for monitoring, search, and logs analytics
4ï¸âƒ£ InfluxDB
â€¢ Time-series database
â€¢ Query language: InfluxQL / Flux
â€¢ Works for metrics like sensor data, server stats, IoT
5ï¸âƒ£ Graphite
â€¢ Legacy time-series database
â€¢ Query language: Graphite functions
â€¢ Still used in some legacy monitoring environments
6ï¸âƒ£ MySQL / PostgreSQL / MSSQL
â€¢ SQL databases
â€¢ Query language: SQL
â€¢ Useful for business metrics, custom dashboards
7ï¸âƒ£ Cloud Monitoring Systems
â€¢ AWS CloudWatch
â€¢ Google Stackdriver (GCP Monitoring)
â€¢ Azure Monitor
â€¢ Query metrics, logs, and alarms from cloud providers
8ï¸âƒ£ Tracing & Observability
â€¢ Jaeger (distributed tracing)
â€¢ Tempo (traces)
â€¢ OpenTelemetry collectors
9ï¸âƒ£ Others
â€¢ JSON API
â€¢ Microsoft SQL Server
â€¢ ClickHouse, Loki, Tempo
â€¢ Many community plugins for new sources
24. How does Grafana query Prometheus?
ðŸ”¹ How Grafana Queries Prometheus
1. Grafana connects to Prometheus as a data source
    â—¦ In Grafana, you configure Prometheus with:
        â–ª URL (e.g., http://prometheus:9090)
        â–ª Access method (proxy or direct)
        â–ª Optional authentication (basic auth, TLS)
2. Panels use PromQL queries
    â—¦ Each panel has a query editor
    â—¦ You write PromQL to fetch metrics
    â—¦ Grafana sends the query via HTTP to Prometheusâ€™s /api/v1/query or /api/v1/query_range endpoints
3. Prometheus executes the query
    â—¦ Returns JSON results:
        â–ª Time series
        â–ª Labels
        â–ª Values
    â—¦ Can be:
        â–ª Instant query â†’ single timestamp
        â–ª Range query â†’ series over a time range
4. Grafana renders the panel
    â—¦ Converts JSON results into:
        â–ª Graphs
        â–ª Tables
        â–ª Singlestat panels
        â–ª Heatmaps, etc.
25. Variables in Grafana â€” why are they useful?
ðŸ”¹ What Are Variables in Grafana?
Variables are dynamic placeholders you can use in queries, panel titles, or dashboards.
â€¢ Defined at the dashboard level
â€¢ Can be reused across multiple panels
â€¢ Can be dropdowns, multi-select, or custom values
ðŸ”¹ Why Variables Are Useful
1ï¸âƒ£ Dynamic Dashboards
â€¢ One dashboard can monitor multiple clusters, services, or environments
â€¢ Instead of creating separate dashboards, you use a variable:

rate(http_requests_total{job="$job"}[5m])

â€¢ $job can be a dropdown with all jobs (services) discovered by Prometheus
2ï¸âƒ£ Reduce Dashboard Duplication
â€¢ Variables allow reuse of panels
â€¢ Example: same CPU panel works for multiple nodes
3ï¸âƒ£ Interactive Filtering
â€¢ Users can select:
    â—¦ Service
    â—¦ Namespace
    â—¦ Pod
    â—¦ Region
â€¢ Panels automatically update with the selection
4ï¸âƒ£ Powerful Query Composition
â€¢ Variables can be used in PromQL, table queries, or annotations
â€¢ Enable dashboards to adapt to changing infrastructure
5ï¸âƒ£ Multi-Select & Regex
â€¢ Variables can select multiple values at once
â€¢ Regex can filter which targets to show
26. How do you create alerts in Grafana?
ðŸ”¹ How Alerts Work in Grafana
Grafana alerts are triggered based on data from panels, usually from metrics in Prometheus.
â€¢ Alerts evaluate a condition on a time-series query.
â€¢ If the condition is met, Grafana sends notifications to configured channels.
â€¢ Grafana supports alert rules at the panel level or centralized alerting.
ðŸ”¹ Steps to Create Alerts in Grafana
1ï¸âƒ£ Configure a Data Source
â€¢ Ensure Prometheus (or another metric source) is added to Grafana.
2ï¸âƒ£ Create a Panel
â€¢ Create a graph or visualization panel displaying the metric you want to monitor.
3ï¸âƒ£ Define an Alert Rule
â€¢ In the panel:
    â—¦ Go to the Alert tab
    â—¦ Click â€œCreate Alertâ€
â€¢ Set:
    â—¦ Condition (threshold, increase, rate)
    â—¦ Evaluation interval (e.g., every 1 minute)
    â—¦ For duration (e.g., fire alert only if condition persists 5 min)
Example condition:

avg(rate(http_requests_total{job="auth-service"}[5m])) > 100

â€¢ Fires alert if average request rate > 100 for 5 minutes
4ï¸âƒ£ Set Notification Channel
â€¢ Configure channels for alert delivery:
    â—¦ Slack
    â—¦ Email
    â—¦ PagerDuty
    â—¦ Webhook
â€¢ Link the alert rule to the notification channel.
5ï¸âƒ£ Test & Save
â€¢ Verify the alert triggers correctly using Test Rule
â€¢ Save the dashboard and alert configuration
ðŸ”¹ Advanced Options
â€¢ Multiple conditions per alert (AND/OR)
â€¢ Annotations on graphs when alerts fire
â€¢ Templated dashboards: Alerts dynamically change based on variables
â€¢ Contact policies: Route alerts to different teams
27. What is a Grafana annotation?
ðŸ”¹ What Is a Grafana Annotation?
Annotations are markers or notes on a Grafana graph/panel that indicate events or incidents.
â€¢ They overlay on time-series graphs.
â€¢ Provide context to metrics.
â€¢ Can be manual or automatic.
ðŸ”¹ Types of Annotations
1ï¸âƒ£ Manual Annotations
â€¢ Added directly in the Grafana UI.
â€¢ Example: â€œDeployment startedâ€ at 10:05 AM.
â€¢ Useful for marking deployments, incidents, or maintenance.
2ï¸âƒ£ Automatic / Query-Based Annotations
â€¢ Fetched from a data source (Prometheus, Elasticsearch, MySQL, etc.)
â€¢ Example: Prometheus alert firing â†’ annotation automatically appears on the graph.
â€¢ Useful for linking metrics with events programmatically.
ðŸ”¹ Why Annotations Are Useful
1. Provide Context
    â—¦ Helps explain spikes or anomalies in metrics.
    â—¦ Example: â€œCPU spike corresponds to deployment rollout.â€
2. Correlation Between Metrics and Events
    â—¦ Correlate alerts, deployments, or incidents with time-series data.
3. Audit / Troubleshooting
    â—¦ Helps teams investigate incidents faster.
ðŸ”¹ Example Use Case
1. Prometheus fires an alert: High CPU usage
2. Grafana query-based annotation appears at alert time on the dashboard
3. Overlay shows: deployment, crash, or incident
4. Engineers immediately understand why CPU spiked
28. How do you manage dashboards as code?
ðŸ”¹ What Does â€œDashboards as Codeâ€ Mean?
Dashboards as code means managing Grafana dashboards in a version-controlled format (JSON, YAML, or via tooling) rather than manually through the UI.
â€¢ Dashboards are stored in Git
â€¢ Changes are tracked and auditable
â€¢ Dashboards are automatically deployed to Grafana
ðŸ”¹ Ways to Manage Grafana Dashboards as Code
1ï¸âƒ£ JSON Export / Import
â€¢ Grafana dashboards are stored as JSON
â€¢ Export a dashboard â†’ store in Git â†’ import to Grafana
â€¢ Simple but manual deployment
2ï¸âƒ£ Grafana Provisioning
â€¢ Grafana supports provisioning dashboards via YAML/JSON
â€¢ Place dashboard files in /etc/grafana/provisioning/dashboards/
â€¢ Grafana loads dashboards on startup automatically
â€¢ Example:

apiVersion: 1
providers:
  - name: 'default'
    orgId: 1
    folder: ''
    type: file
    options:
      path: /var/lib/grafana/dashboards

3ï¸âƒ£ Grafana Terraform Provider
â€¢ Use Terraform to define dashboards as code
â€¢ Example:

resource "grafana_dashboard" "app_dashboard" {
  config_json = file("${path.module}/dashboards/app.json")
}

â€¢ Supports full lifecycle management: create, update, delete
â€¢ Ideal for CI/CD pipelines
4ï¸âƒ£ Grafana API
â€¢ Grafana provides a REST API to create/update dashboards
â€¢ Can be automated via scripts or CI/CD pipelines
5ï¸âƒ£ Grafonnet / Jsonnet
â€¢ Jsonnet library to generate dynamic, templated dashboards
â€¢ Useful for multi-environment dashboards
ðŸ”¹ Benefits of Managing Dashboards as Code
1. Version Control
    â—¦ Track changes, rollbacks, and approvals in Git
2. Consistency
    â—¦ Same dashboards deployed across environments
3. Automation
    â—¦ CI/CD pipelines can deploy dashboards automatically
4. Collaboration
    â—¦ Teams can review changes via pull requests
29. Difference between table panel and time-series panel?
FeatureTime-Series PanelTable PanelPurposeVisualize metrics over timeDisplay metrics or logs in a tabular formatData TypeTime-series (timestamps + values)Tabular data (rows and columns)Best ForTrends, graphs, spikes, latency, CPU/memory usageDetailed lists, status tables, top N metrics, logsVisualizationLine, bar, area, heatmapTable with sortable columns, thresholds, linksQuery Examplerate(http_requests_total[5m])topk(5, sum(rate(http_requests_total[5m])) by (service))Annotations / AlertsCan overlay annotations, visualize thresholdsAlerts can be applied, but more for visual reporting
30. How do you handle multi-tenant dashboards?
ðŸ”¹ What Is a Multi-Tenant Dashboard?
Multi-tenant dashboards allow multiple users or teams (â€œtenantsâ€) to:
â€¢ Use the same Grafana instance
â€¢ Have separate dashboards, data sources, and permissions
â€¢ Avoid interfering with each otherâ€™s metrics or alerts
ðŸ”¹ Strategies to Handle Multi-Tenancy in Grafana
1ï¸âƒ£ Grafana Organizations
â€¢ Grafana natively supports organizations.
â€¢ Each tenant/team can have:
    â—¦ Separate dashboards
    â—¦ Separate data sources
    â—¦ Separate users
â€¢ Users belong to one or more orgs, with role-based permissions (Admin, Editor, Viewer)
Example:

Org A â†’ Dev dashboards + Prometheus data source
Org B â†’ QA dashboards + Prometheus data source

2ï¸âƒ£ Folder & Dashboard Permissions
â€¢ Dashboards can be grouped in folders
â€¢ Set read/write permissions per folder for teams
3ï¸âƒ£ Data Source Isolation
â€¢ Each tenant can have dedicated data sources
â€¢ Useful if tenants have:
    â—¦ Separate Prometheus instances
    â—¦ Separate clusters
â€¢ Prevents one team from querying another teamâ€™s data
4ï¸âƒ£ Dashboard Variables / Templating
â€¢ Use templated variables for shared dashboards across tenants
â€¢ Example:

rate(http_requests_total{cluster="$cluster"}[5m])

    â—¦ $cluster variable changes per tenant
â€¢ Reduces dashboard duplication while still isolating views
5ï¸âƒ£ API & Automation
â€¢ Manage tenants programmatically with Grafana API or Terraform provider
â€¢ Automate:
    â—¦ Dashboard provisioning
    â—¦ Folder & permission assignment
    â—¦ Data source creation per tenant
31. Grafana performance tuning techniques?
ðŸ”¹ Key Areas for Grafana Performance Tuning
1ï¸âƒ£ Optimize Data Sources & Queries
â€¢ Use Prometheus recording rules instead of heavy PromQL queries in dashboards
    â—¦ Precompute rates, aggregates, or percentiles
â€¢ Avoid count/sum on massive time-series without aggregation
â€¢ Use time ranges and resolution carefully
    â—¦ Reduce panel queries frequency (Min interval setting)
2ï¸âƒ£ Reduce Panel & Dashboard Load
â€¢ Avoid too many panels in a single dashboard
â€¢ Combine metrics where possible
â€¢ Use templating/variables carefully:
    â—¦ Multi-select variables with many values can explode queries
â€¢ Use row collapse or multiple dashboards instead of one mega-dashboard
3ï¸âƒ£ Caching & Refresh Intervals
â€¢ Set reasonable dashboard refresh intervals (e.g., 30sâ€“1min)
â€¢ Enable query caching (depends on data source, e.g., Prometheus proxy caching)
4ï¸âƒ£ Grafana Server Tuning
â€¢ Increase Grafana server resources (CPU/memory) for large installations
â€¢ Use Grafana 9+ or newer versions for better rendering performance
â€¢ Enable parallel queries (concurrent_render_limit) for panels with many queries
5ï¸âƒ£ Use Provisioned Dashboards / Dashboards as Code
â€¢ Pre-load dashboards via provisioning instead of creating them in the UI
â€¢ Reduces UI rendering overhead
6ï¸âƒ£ Scale Grafana for Multi-User Access
â€¢ For large teams or enterprise setups:
    â—¦ Run multiple Grafana instances behind a load balancer
    â—¦ Use Redis or database caching for session & alert storage
â€¢ Use Grafana Enterprise for built-in scaling features (optional)
7ï¸âƒ£ Minimize Heavy Visualizations
â€¢ Avoid too many heatmaps or high-cardinality visualizations in one panel
â€¢ Use singlestat or table panels for high-density metrics
8ï¸âƒ£ Alerting Performance
â€¢ Use Prometheus alerting rules instead of Grafana alerts when possible
â€¢ Alerts in Grafana query Prometheus frequently â†’ can increase load
32. How do you integrate Grafana with authentication systems?
ðŸ”¹ Common Methods to Integrate Authentication
1ï¸âƒ£ LDAP / Active Directory
â€¢ Grafana connects to LDAP/AD server
â€¢ Users can log in with corporate credentials
â€¢ Supports mapping LDAP groups to Grafana roles (Admin, Editor, Viewer)
Example Config:

[auth.ldap]
enabled = true
config_file = /etc/grafana/ldap.toml
allow_sign_up = true

2ï¸âƒ£ OAuth / SSO
â€¢ Grafana supports OAuth2 providers: Google, GitHub, Okta, Azure AD
â€¢ Users log in via the provider
â€¢ Roles can be assigned based on claims or groups
â€¢ Great for multi-tenant environments or cloud setups
Example:

[auth.google]
enabled = true
client_id = YOUR_CLIENT_ID
client_secret = YOUR_CLIENT_SECRET
scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email

3ï¸âƒ£ SAML
â€¢ Enterprise SSO via SAML providers (Okta, OneLogin, Azure AD)
â€¢ Centralized authentication for Grafana
â€¢ Supports role mapping from SAML attributes
4ï¸âƒ£ Proxy Authentication
â€¢ Reverse proxy handles auth (e.g., Nginx, Traefik)
â€¢ Grafana trusts headers like X-WEBAUTH-USER
â€¢ Useful for internal deployments behind a corporate gateway
5ï¸âƒ£ Role-Based Access Control (RBAC)
â€¢ After authentication, assign roles: Admin, Editor, Viewer
â€¢ Can be org-level or folder-level
â€¢ Combined with multi-tenant dashboards for access isolation
33. How do you restrict dashboard access?
ðŸ”¹ Ways to Restrict Dashboard Access in Grafana
1ï¸âƒ£ Organizations
â€¢ Grafana supports multiple organizations.
â€¢ Users belong to one or more orgs.
â€¢ Dashboards are isolated per organization.
Example:

Org A â†’ Dev dashboards
Org B â†’ QA dashboards

2ï¸âƒ£ Folders and Folder Permissions
â€¢ Dashboards can be grouped in folders.
â€¢ Permissions can be set per folder:
    â—¦ Viewer â†’ can only view dashboards
    â—¦ Editor â†’ can edit dashboards
    â—¦ Admin â†’ full control
â€¢ Useful for separating dashboards by team, environment, or service.
3ï¸âƒ£ Dashboard Permissions
â€¢ Grafana allows dashboard-level permissions.
â€¢ You can explicitly allow or deny:
    â—¦ Individual users
    â—¦ Teams
    â—¦ Roles (Viewer, Editor, Admin)
Example:

Dashboard X â†’ Editors: Team A, Viewers: Team B

4ï¸âƒ£ Role-Based Access Control (RBAC)
â€¢ Roles can be applied:
    â—¦ Organization level
    â—¦ Folder level
    â—¦ Dashboard level
â€¢ Admin, Editor, Viewer roles control what actions users can perform.
5ï¸âƒ£ Data Source Restrictions
â€¢ Limit which data sources a user or team can access.
â€¢ Users with no access to a data source cannot view dashboards using that source.
6ï¸âƒ£ Authentication Integration
â€¢ Use SSO / LDAP / OAuth / SAML to map users to Grafana roles or teams.
â€¢ Combine with folder and dashboard permissions for fine-grained access control.
7ï¸âƒ£ Proxy or Network Restrictions
â€¢ Restrict access to Grafana using network policies, firewalls, or reverse proxies.
â€¢ Useful for internal dashboards or multi-tenant isolation.
âš™ï¸ Scenario-Based Questions
34. CPU usage is spiking â€” how do you debug using Prometheus & Grafana?
ðŸ”¹ Step 1: Confirm the Spike
1. Open Grafana dashboard for CPU metrics.
2. Check time-series graphs for the spike:
    â—¦ Node CPU usage (node_cpu_seconds_total)
    â—¦ Pod/container CPU usage (container_cpu_usage_seconds_total) if Kubernetes
3. Verify if the spike is sustained or a short burst.
ðŸ’¡ Look at raw metrics and aggregated values.
ðŸ”¹ Step 2: Identify the Scope
â€¢ Is the spike system-wide or application-specific?
â€¢ Use PromQL queries to break it down:
Example: Per Node CPU Usage

100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

Example: Per Pod in Kubernetes

sum by(pod) (rate(container_cpu_usage_seconds_total[5m]))

â€¢ This helps isolate which nodes or pods are contributing most to CPU usage.
ðŸ”¹ Step 3: Check Process / Service Contribution
â€¢ If Kubernetes, use label filters:

sum by(namespace, pod, container) (rate(container_cpu_usage_seconds_total[5m]))

â€¢ Identify top CPU-consuming pods/services using topk():

topk(5, sum by(pod) (rate(container_cpu_usage_seconds_total[5m])))

â€¢ This shows which services or pods are causing the spike.
ðŸ”¹ Step 4: Correlate with Other Metrics
1. Memory usage (container_memory_usage_bytes) â†’ Is CPU spike correlated with memory pressure?
2. Disk / I/O (node_disk_io_time_seconds_total) â†’ Are there I/O bottlenecks?
3. Network metrics â†’ Is network saturation affecting CPU?
4. Application metrics â†’ Requests per second, error rates
ðŸ’¡ Use Grafana panels and annotations to correlate events (deployments, cron jobs, alerts).
ðŸ”¹ Step 5: Check Events / Logs
â€¢ Use Loki or other log data source in Grafana:
    â—¦ Filter logs for the time range of the CPU spike
    â—¦ Look for errors, slow queries, or heavy tasks
ðŸ”¹ Step 6: Drill Down / Alert Investigation
â€¢ Check Prometheus alerting rules (if CPU threshold alerts exist)
â€¢ Check if the spike triggered Alertmanager notifications
â€¢ Correlate with dashboards: which pods, namespaces, or nodes were affected
ðŸ”¹ Step 7: Long-Term Analysis
â€¢ Compare spike with historical trends:
    â—¦ Is it daily/hourly load pattern?
    â—¦ Are specific jobs or cron tasks causing the spike?
â€¢ Use Prometheus recording rules to pre-aggregate metrics for easier comparison.
ðŸ”¹ Step 8: Take Action
â€¢ Based on analysis, options may include:
    â—¦ Scaling pods or nodes
    â—¦ Optimizing the application/service
    â—¦ Throttling batch jobs
    â—¦ Scheduling heavy tasks at off-peak times
35. A Prometheus alert is firing but everything looks fine â€” what do you do?
ðŸ”¹ Step 1: Donâ€™t Panic â€” Confirm the Alert
â€¢ Check Alertmanager or Grafana notification
â€¢ Look at the alert name, labels, and firing time
â€¢ Note which targets/instances triggered the alertExample: HighCPUUsage alert fired for pod auth-service
ðŸ”¹ Step 2: Check the Query in Prometheus
1. Go to Prometheus â€œAlertsâ€ or â€œGraphâ€ tab
2. Copy the PromQL expression from the alert rule
3. Run it manually for the same time range
â€¢ Check if the query is currently evaluating to true
â€¢ If the metric looks normal, it may be a query or threshold issue
ðŸ”¹ Step 3: Check Evaluation Interval & â€˜Forâ€™ Duration
â€¢ Alerts in Prometheus can have a for clause:

alert: HighCPU
expr: rate(cpu_usage_seconds_total[5m]) > 80
for: 5m

â€¢ If spike was shorter than 5 minutes, alert may still fire due to evaluation delay
â€¢ Similarly, very short intervals may cause flapping alerts
ðŸ”¹ Step 4: Look for Stale or Missing Metrics
â€¢ Metrics may be temporarily missing or delayed due to:
    â—¦ Node exporter scrape failure
    â—¦ Pod restart
    â—¦ Network issues
â€¢ Prometheus treats absent metrics as zero or may trigger alert depending on alert logic
Check with PromQL:

absent(cpu_usage_seconds_total{job="auth-service"})

ðŸ”¹ Step 5: Check for High Cardinality or Label Changes
â€¢ Alerts may fire unexpectedly if labels change or new pods are added
â€¢ Example: rate(cpu_usage_seconds_total[5m]) with per-pod aggregation may include new pods temporarily
ðŸ”¹ Step 6: Check Alertmanager Deduplication & Silences
â€¢ Alert may appear firing in Alertmanager, but:
    â—¦ It could be a duplicate firing
    â—¦ A previous alert was silenced or pending
â€¢ Verify Alertmanager UI for pending or silenced alerts
ðŸ”¹ Step 7: Check Grafana or External Dashboards
â€¢ Cross-check metrics in Grafana
â€¢ Compare time ranges used in alert rule vs dashboard
â€¢ Confirm the spike or anomaly exists
ðŸ”¹ Step 8: Adjust Alert Rule if Needed
â€¢ Increase for duration to avoid flapping
â€¢ Add unless absent() clauses to prevent firing on missing metrics
â€¢ Refine PromQL expression for better accuracy
ðŸ”¹ Step 9: Document and Monitor
â€¢ Log the false positive and adjust rules if needed
â€¢ Set up dashboard panels to cross-verify alerts before notifying team
36. Metrics are missing for a service â€” troubleshooting steps?
ðŸ”¹ Step 1: Verify the Service is Up
â€¢ Ensure the service or exporter is running and healthy
â€¢ Check:
    â—¦ Service pod/container status (kubectl get pods)
    â—¦ Logs for errors
    â—¦ Network connectivity
ðŸ”¹ Step 2: Check the Exporter
â€¢ If Prometheus scrapes metrics via an exporter (Node Exporter, App Exporter):
    â—¦ Is the exporter running?
    â—¦ Can you access the /metrics endpoint directly?

curl http://<exporter-host>:9100/metrics

    â—¦ Ensure the metrics for your service are actually exposed
ðŸ”¹ Step 3: Verify Prometheus Scrape Config
â€¢ Check prometheus.yml or config map:
    â—¦ Job name exists
    â—¦ Targets include the service
    â—¦ Correct port, path (/metrics)
â€¢ Access Prometheus Targets page (/targets)
    â—¦ Check if the target is UP or DOWN
    â—¦ Look at last scrape time
ðŸ”¹ Step 4: Check Labels and Relabeling
â€¢ Misconfigured labels or relabeling can cause metrics to be missing
â€¢ Ensure metric name and labels match Prometheus queries
â€¢ Example: If a service exposes http_requests_total, check that your query is correct:

rate(http_requests_total{job="my-service"}[5m])

ðŸ”¹ Step 5: Check Prometheus Logs
â€¢ Prometheus logs may indicate:
    â—¦ Scrape failures
    â—¦ Connection timeouts
    â—¦ TLS/authentication errors
ðŸ”¹ Step 6: Check Metric Retention & Query Time Range
â€¢ Ensure query is within retention period
â€¢ Metrics older than --storage.tsdb.retention.time may not appear
â€¢ Adjust time range in Grafana accordingly
ðŸ”¹ Step 7: Check for High Cardinality / Missing Series
â€¢ Sometimes metrics exist but with unexpected labels
â€¢ Use Prometheus query to list series:

{job="my-service"}

â€¢ Verify that expected series appear
ðŸ”¹ Step 8: Network / Firewall Checks
â€¢ Ensure Prometheus can reach the service/exporter over network
â€¢ Check firewall rules, service endpoints, security groups
ðŸ”¹ Step 9: Check for Application Changes
â€¢ Metrics may have been removed or renamed in a new release
â€¢ Check changelogs or application code for exposed metrics
ðŸ”¹ Step 10: Test in Isolation
â€¢ Temporarily use direct Prometheus query or curl to fetch metrics
â€¢ Verify that the service produces metrics correctly
37. How would you monitor a batch job vs a long-running service?
ðŸ”¹ Monitoring a Long-Running Service
Examples: Web service, API server, microservice.
Key Metrics to Monitor
1. CPU / Memory / Disk / Network
    â—¦ node_cpu_seconds_total, container_memory_usage_bytes, container_fs_usage_bytes
2. Application metrics
    â—¦ Request rate, latency, error rate (http_requests_total, http_request_duration_seconds)
3. Availability
    â—¦ Uptime, pod or process status
4. Custom business metrics
    â—¦ Orders processed, queue depth
Monitoring Approach
â€¢ Scrape metrics continuously via Prometheus exporters or instrumentation libraries (client libraries in Go, Java, Python, etc.)
â€¢ Dashboards in Grafana with time-series panels
â€¢ Alerts via Prometheus Alertmanager
    â—¦ e.g., high CPU for >5 mins, error rate >5%
â€¢ Annotations for deployments or incidents
Key Points:
â€¢ Metrics are continuous â†’ Prometheus works well
â€¢ Use histograms or summaries for latency metrics
â€¢ Monitor trends over time
ðŸ”¹ Monitoring a Batch Job
Examples: Data processing job, nightly ETL, cron job.
Key Metrics / Checks
1. Job completion
    â—¦ Did it finish successfully?
    â—¦ Status: success/failure
2. Duration / Runtime
    â—¦ How long did the job take?
    â—¦ Compare to historical averages
3. Errors or warnings
    â—¦ Failed records, exceptions
4. Resource usage (optional)
    â—¦ CPU, memory, disk I/O during job run
Monitoring Approach
â€¢ Push metrics at the end of the job (Prometheus pushgateway)
    â—¦ Since the job is short-lived, Prometheus canâ€™t scrape it while running
â€¢ Expose metrics via job logs and parse them into Prometheus or Loki
â€¢ Alerts on failures or unusually long runtimes
â€¢ Dashboards can track job history over time
Key Points:
â€¢ Jobs are ephemeral â†’ Prometheus pull model wonâ€™t see them unless they push metrics
â€¢ Focus is on completion and performance, not continuous resource usage
