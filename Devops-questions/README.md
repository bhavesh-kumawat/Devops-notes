ğŸ”§ CI/CD & Automation
1. Your production deployment failed halfway through. What do you do?
ğŸ”¹ Step 1: Stop the Bleeding (Stabilize First)
â€¢ Pause or abort the deployment pipeline immediately
â€¢ Prevent further changes from being rolled out
â€¢ Communicate in the incident channel: â€œDeployment paused, investigatingâ€First goal is limit blast radius, not fix everything instantly.
ğŸ”¹ Step 2: Assess Impact
Ask:
â€¢ Is the app partially serving traffic?
â€¢ Are users affected or just some instances?
â€¢ Which components are broken? (API, DB, background jobs)
Check:
â€¢ Load balancer health checks
â€¢ Error rates (4xx/5xx)
â€¢ Key business metrics
ğŸ”¹ Step 3: Rollback or Fail Forward
Option A: Rollback (Most Common)
â€¢ Roll back to last known good version
â€¢ Examples:
    â—¦ Blue-Green â†’ switch traffic back to blue
    â—¦ Kubernetes â†’ kubectl rollout undo
    â—¦ ECS â†’ revert task definition
    â—¦ Lambda â†’ rollback to previous version/aliasRollback should be fast, automated, and boring
Option B: Fix Forward (If Safe & Small)
â€¢ Only if:
    â—¦ Issue is minor
    â—¦ No data corruption
    â—¦ Fix is quick and isolated
ğŸ”¹ Step 4: Verify System Health
After rollback:
â€¢ Confirm:
    â—¦ App is stable
    â—¦ Error rates return to normal
    â—¦ Latency and traffic are healthy
â€¢ Keep monitoring for at least 10â€“30 minutes
ğŸ”¹ Step 5: Root Cause Analysis (RCA)
Once stable:
â€¢ Review:
    â—¦ CI/CD logs
    â—¦ Deployment diffs
    â—¦ Config changes
    â—¦ Infra vs app failures
â€¢ Identify:
    â—¦ Why it failed halfway
    â—¦ Why it wasnâ€™t caught earlier
ğŸ”¹ Step 6: Prevent It from Happening Again
Examples:
â€¢ Add:
    â—¦ Pre-deployment checks
    â—¦ Smoke tests
    â—¦ Canary or blue-green deployments
â€¢ Improve:
    â—¦ Rollback automation
    â—¦ Alerting during deploys
    â—¦ Feature flags for risky changes
2. A build works locally but fails in the CI pipeline. How do you debug it?
ğŸ”¹ Step 1: Read the CI Error Carefully (Donâ€™t Guess)
â€¢ Look at:
    â—¦ Exact error message
    â—¦ Which stage failed (build / test / package)
â€¢ Compare:
    â—¦ Local success logs vs CI failure logsMany candidates jump to conclusions â€” good engineers read first.
ğŸ”¹ Step 2: Compare Local vs CI Environments
Most â€œworks locallyâ€ issues come from environment mismatch.
Check:
â€¢ OS (Linux CI vs macOS/Windows local)
â€¢ Language/runtime versions (Java, Node, Python, Go)
â€¢ Package manager versions
â€¢ Environment variables
ğŸ’¡ Fix: Pin versions using:
â€¢ .nvmrc, .python-version
â€¢ Docker
â€¢ Version managers
ğŸ”¹ Step 3: Dependency Issues
Common problems:
â€¢ Missing dependencies in CI
â€¢ package-lock.json / poetry.lock / go.sum not committed
â€¢ Cached dependencies causing conflicts
Actions:
â€¢ Clear CI cache
â€¢ Reinstall dependencies from scratch
â€¢ Run:

npm ci
pip install -r requirements.txt

ğŸ”¹ Step 4: Environment Variables & Secrets
â€¢ Local machine has env vars that CI doesnâ€™t
â€¢ Secrets missing or misconfigured in CI
Check:
â€¢ CI secret manager (GitHub Actions, GitLab CI, Jenkins, etc.)
â€¢ Variable names and scopes
â€¢ Masked secrets vs plaintext values
ğŸ”¹ Step 5: File Paths & Permissions
CI runs in a clean filesystem.
Common failures:
â€¢ Case-sensitive paths (Config.yaml vs config.yaml)
â€¢ Files not committed to repo
â€¢ Permission issues (chmod +x missing)
ğŸ”¹ Step 6: Network / External Services
CI environment may:
â€¢ Block outbound traffic
â€¢ Not have access to:
    â—¦ Databases
    â—¦ Private repos
    â—¦ Internal APIs
Fixes:
â€¢ Mock external services
â€¢ Use test containers
â€¢ Whitelist CI IPs if needed
ğŸ”¹ Step 7: Reproduce CI Locally
Best debugging move ğŸš€
â€¢ Run CI steps locally:

docker run <ci-image>

â€¢ Or:

act   # GitHub Actions

If it fails locally â†’ easier to debug.
ğŸ”¹ Step 8: Add Debugging Output (Temporarily)
â€¢ Print:
    â—¦ Environment variables
    â—¦ Versions
    â—¦ Working directory
â€¢ Enable verbose logging:

set -x

3. How would you design a CI/CD pipeline for a microservices application?
ğŸ”¹ High-Level Goals
For a microservices CI/CD pipeline, the goals are:
â€¢ Independent deployments per service
â€¢ Fast feedback (fail early)
â€¢ Safe releases (no downtime)
â€¢ Easy rollback
â€¢ Consistent environments
ğŸ”¹ High-Level Architecture

Git â†’ CI â†’ Artifact Registry â†’ CD â†’ Kubernetes / ECS â†’ Monitoring

Each microservice has its own pipeline, but follows a standard template.
ğŸ”¹ Step 1: Source Control Strategy
â€¢ One repo per microservice (preferred) or mono-repo with paths
â€¢ Git branching:
    â—¦ main â†’ production
    â—¦ feature/* â†’ feature branches
â€¢ Every commit triggers CI automatically
ğŸ”¹ Step 2: Continuous Integration (CI)
CI Stages (per microservice)
1. Code Checkout
2. Static Checks
    â—¦ Linting
    â—¦ Formatting
    â—¦ Security scanning (SAST)
3. Unit Tests
4. Build Artifact
    â—¦ Docker image
    â—¦ Tag with commit-sha + version
5. Push to Registry
    â—¦ ECR / Docker Hub / Artifact Registry
ğŸ’¡ Fail fast â€” donâ€™t build if tests fail.
ğŸ”¹ Step 3: Artifact Management
â€¢ Store immutable artifacts:
    â—¦ Docker images
    â—¦ Helm charts
â€¢ Never rebuild the same artifact in CDâ€œBuild once, deploy many timesâ€
ğŸ”¹ Step 4: Continuous Deployment (CD)
Deployment Flow
1. Deploy to Dev
2. Automated integration tests
3. Deploy to Staging
4. Manual approval (optional)
5. Deploy to Production
Deployment Strategies
â€¢ Blue-Green for critical services
â€¢ Canary for high-traffic APIs
â€¢ Rolling updates for low-risk services
ğŸ”¹ Step 5: Configuration & Secrets
â€¢ Config:
    â—¦ ConfigMaps / Parameter Store
â€¢ Secrets:
    â—¦ Secrets Manager / Vault / Kubernetes Secrets
â€¢ Environment-specific configs, same image
ğŸ”¹ Step 6: Infrastructure as Code
â€¢ Provision infra using:
    â—¦ Terraform / CloudFormation
â€¢ Kubernetes:
    â—¦ Helm or Kustomize
â€¢ Version-controlled infrastructure
ğŸ”¹ Step 7: Observability & Feedback
â€¢ Monitoring: Prometheus + Grafana / CloudWatch
â€¢ Logging: ELK / CloudWatch Logs
â€¢ Tracing: X-Ray / Jaeger
â€¢ Alerts tied to deployments
ğŸ”¹ Step 8: Rollback Strategy
â€¢ Rollback by:
    â—¦ Reverting Helm release
    â—¦ Switching traffic back (blue-green)
    â—¦ Redeploying previous image tag
Rollback must be:
â€¢ Fast
â€¢ Automated
â€¢ Safe
ğŸ”¹ Security Built-In (Shift Left)
â€¢ Dependency scanning
â€¢ Image scanning
â€¢ Secrets scanning
â€¢ Least-privilege IAM roles
4. Your pipeline is too slow and blocking releases. How do you optimize it?
ğŸ”¹ Step 1: Find Where Time Is Actually Spent
First rule: donâ€™t optimize blindly.
â€¢ Measure:
    â—¦ Build time
    â—¦ Test time
    â—¦ Image build & push
    â—¦ Deployment time
â€¢ Use pipeline timing metrics or logsâ€œI start by identifying the slowest stages before making changes.â€
ğŸ”¹ Step 2: Fail Fast (Shift Left)
â€¢ Run linting & unit tests first
â€¢ Stop the pipeline early if something fails
â€¢ Donâ€™t build Docker images if tests fail
ğŸ’¡ Saves time and money.
ğŸ”¹ Step 3: Parallelize Aggressively
â€¢ Run:
    â—¦ Unit tests in parallel
    â—¦ Integration tests in parallel
    â—¦ Independent microservice pipelines independently
Examples:
â€¢ Test sharding
â€¢ Matrix buildsSerial pipelines are the biggest CI killer.
ğŸ”¹ Step 4: Use Caching Correctly (But Carefully)
Cache:
â€¢ Dependencies (node_modules, Maven, pip)
â€¢ Docker layers
â€¢ Build artifacts
Avoid:
â€¢ Over-caching â†’ flaky builds
â€¢ Cache invalidation bugs
ğŸ”¹ Step 5: Optimize Docker Builds
â€¢ Use multi-stage Docker builds
â€¢ Minimize layers
â€¢ Use smaller base images (Alpine, distroless)
â€¢ Reorder Dockerfile:
    â—¦ Copy dependency files first
    â—¦ Install deps once
ğŸ”¹ Step 6: Reduce Test Scope Smartly
â€¢ Run:
    â—¦ Full test suite on main
    â—¦ Targeted tests on feature branches
â€¢ Use:
    â—¦ Contract tests instead of heavy E2E tests
    â—¦ Nightly full regression tests
ğŸ”¹ Step 7: Optimize Deployment Strategy
â€¢ Use:
    â—¦ Rolling updates
    â—¦ Canary instead of full redeploys
â€¢ Avoid redeploying unchanged services
â€¢ Use immutable artifacts
ğŸ”¹ Step 8: Scale CI Infrastructure
â€¢ Increase:
    â—¦ CI runners
    â—¦ CPU / memory for build agents
â€¢ Use autoscaling runnersA slow pipeline is often an underpowered pipeline.
ğŸ”¹ Step 9: Introduce Async & Decoupling
â€¢ Make non-critical checks async:
    â—¦ Security scans
    â—¦ Performance tests
â€¢ Donâ€™t block releases unnecessarily
â˜ï¸ Cloud & Infrastructure
5. Traffic suddenly spikes 10Ã—. Your app becomes slow. What steps do you take?
ğŸ”´ Phase 1: Stabilize the System (First 5â€“10 Minutes)
1ï¸âƒ£ Confirm the Spike Is Real
â€¢ Check:
    â—¦ Load balancer request count
    â—¦ Traffic sources (organic vs bots)
â€¢ Validate itâ€™s not a monitoring glitchDonâ€™t fight ghosts.
2ï¸âƒ£ Protect the User Experience
â€¢ Enable / tighten rate limiting
â€¢ Turn on:
    â—¦ API Gateway throttling
    â—¦ WAF rules (bot / abuse protection)
â€¢ Return graceful degradation:
    â—¦ Feature flags off
    â—¦ Disable non-critical functionality
âš¡ Phase 2: Scale Fast (Next 10â€“20 Minutes)
3ï¸âƒ£ Scale the App Layer
â€¢ Trigger or increase:
    â—¦ Auto Scaling Groups
    â—¦ Kubernetes HPA
    â—¦ ECS service desired count
â€¢ Scale horizontally firstVertical scaling is slower and riskier during incidents.
4ï¸âƒ£ Check the Load Balancer
â€¢ Ensure:
    â—¦ Healthy targets
    â—¦ No connection saturation
â€¢ Enable connection draining if needed
ğŸ§  Phase 3: Identify Bottlenecks
5ï¸âƒ£ Identify the Limiting Resource
Check dashboards:
â€¢ CPU / memory
â€¢ Thread pools
â€¢ DB connections
â€¢ Queue depth
â€¢ Cache hit ratio
Ask:
â€¢ App layer?
â€¢ Database?
â€¢ Cache?
â€¢ External API?
6ï¸âƒ£ Database & Cache Protection
â€¢ Enable / expand:
    â—¦ Read replicas
    â—¦ Query caching
    â—¦ Connection pooling
â€¢ Apply timeouts and circuit breakersDatabases are usually the first to melt.
ğŸ” Phase 4: Optimize Traffic Flow
7ï¸âƒ£ Use Caching Aggressively
â€¢ Enable:
    â—¦ CDN (CloudFront)
    â—¦ Application cache (Redis / ElastiCache)
â€¢ Cache:
    â—¦ Static assets
    â—¦ Read-heavy API responses
8ï¸âƒ£ Queue Where Possible
â€¢ Offload:
    â—¦ Emails
    â—¦ Reports
    â—¦ Background jobs
â€¢ Use:
    â—¦ SQS / Kafka
    â—¦ Async processing
ğŸ“ˆ Phase 5: Observe & Communicate
9ï¸âƒ£ Monitor in Real Time
â€¢ Error rate
â€¢ Latency
â€¢ Saturation metrics
â€¢ Watch for cascading failures
ğŸ”Ÿ Communicate Clearly
â€¢ Update:
    â—¦ Incident channel
    â—¦ Stakeholders
â€¢ Set expectationsSilence causes more damage than outages.
ğŸ§ª Phase 6: After the Incident
11ï¸âƒ£ Post-Incident Review
â€¢ Root cause
â€¢ Why scaling wasnâ€™t fast enough
â€¢ What broke first
12ï¸âƒ£ Prevent Recurrence
â€¢ Load testing
â€¢ Autoscaling tuning
â€¢ Capacity planning
â€¢ Better caching strategy
6. One Availability Zone goes down. How do you design for high availability?
ğŸ”¹ 1. Multi-AZ Architecture (Baseline)
Compute
â€¢ Run workloads across multiple Availability Zones
â€¢ Use:
    â—¦ Auto Scaling Groups spanning â‰¥2 AZs
    â—¦ ECS / EKS with multi-AZ scheduling
Load Balancing
â€¢ Use ALB / NLB across AZs
â€¢ Health checks automatically route traffic away from failed AZs
ğŸ”¹ 2. Stateless Application Design
â€¢ Keep app instances stateless
â€¢ Store state in:
    â—¦ RDS / DynamoDB
    â—¦ S3
    â—¦ ElastiCache (with replication)Stateless apps recover instantly when instances are replaced.
ğŸ”¹ 3. Highly Available Data Layer
Databases
â€¢ RDS / Aurora Multi-AZ
    â—¦ Automatic failover
â€¢ DynamoDB
    â—¦ Multi-AZ by default
Caching
â€¢ ElastiCache with replication groups
â€¢ Primary + replicas across AZs
ğŸ”¹ 4. Storage Resilience
â€¢ S3 â†’ multi-AZ by default
â€¢ EBS
    â—¦ Use snapshots
    â—¦ Attach only within AZ, but recover quickly
â€¢ EFS
    â—¦ Multi-AZ file system
ğŸ”¹ 5. Health Checks & Auto Recovery
â€¢ Load balancer health checks
â€¢ Auto Scaling replaces unhealthy instances
â€¢ Kubernetes:
    â—¦ Liveness & readiness probes
    â—¦ Pod rescheduling to healthy AZs
ğŸ”¹ 6. Traffic & DNS Strategy
â€¢ Route 53 health checks
â€¢ Optional:
    â—¦ Failover routing
    â—¦ Weighted routing (for multi-region)
ğŸ”¹ 7. Deployment Strategy Matters
â€¢ Rolling, Blue-Green, or Canary
â€¢ Never deploy all AZs at once
â€¢ Always keep at least one AZ serving traffic
ğŸ”¹ 8. Test AZ Failures (Very Important)
â€¢ Simulate AZ failures:
    â—¦ Disable subnets
    â—¦ Kill nodes
â€¢ Validate:
    â—¦ Traffic shifts
    â—¦ Recovery time (RTO)
    â—¦ Data integrity (RPO)HA you donâ€™t test is just hope.
7. You need to reduce cloud costs without affecting performance. What do you do?
ğŸ§  Core PrincipleReduce waste first. Optimize later. Never degrade performance.
ğŸ”¹ Step 1: Get Visibility (You Canâ€™t Fix What You Canâ€™t See)
â€¢ Use:
    â—¦ AWS Cost Explorer
    â—¦ Cost & Usage Reports (CUR)
    â—¦ Per-service and per-team tagging
â€¢ Identify:
    â—¦ Top 5 cost drivers
    â—¦ Underutilized resources
ğŸ”¹ Step 2: Eliminate Obvious Waste (Fast Wins)
Compute
â€¢ Stop or downsize:
    â—¦ Idle EC2 instances
    â—¦ Unused ASGs
â€¢ Right-size instances using:
    â—¦ CPU / memory metrics
â€¢ Remove:
    â—¦ Unattached EBS volumes
    â—¦ Old snapshots
ğŸ”¹ Step 3: Optimize Scaling (Biggest Impact)
â€¢ Use Auto Scaling aggressively
â€¢ Scale based on:
    â—¦ Request count
    â—¦ Queue depth
    â—¦ Latency (not just CPU)
â€¢ Reduce baseline capacity off-peakPaying for idle capacity is the #1 cost leak.
ğŸ”¹ Step 4: Use the Right Pricing Models
â€¢ Reserved Instances / Savings Plans for steady workloads
â€¢ Spot Instances for:
    â—¦ Batch jobs
    â—¦ CI runners
    â—¦ Non-critical services
â€¢ Keep critical workloads on On-Demand
ğŸ”¹ Step 5: Improve Performance to Reduce Cost
Counterintuitive but true ğŸ‘‡
â€¢ Faster apps = fewer resources
Actions:
â€¢ Add caching (Redis, CloudFront)
â€¢ Optimize DB queries
â€¢ Reduce chatty APIs
ğŸ”¹ Step 6: Storage Optimization
â€¢ Move infrequently accessed data to:
    â—¦ S3 IA / Glacier
â€¢ Enable:
    â—¦ S3 lifecycle policies
    â—¦ Log retention policies
â€¢ Compress logs
ğŸ”¹ Step 7: Network & Data Transfer Costs
â€¢ Use:
    â—¦ VPC endpoints (avoid NAT data charges)
    â—¦ CloudFront to reduce egress
â€¢ Keep traffic inside the same region/AZ
ğŸ”¹ Step 8: CI/CD & Dev/Test Cost Control
â€¢ Auto-shutdown non-prod environments
â€¢ Use smaller instances for test workloads
â€¢ Limit parallel CI runners
ğŸ”¹ Step 9: Guardrails & Automation
â€¢ Budgets + alerts
â€¢ Policy-as-code:
    â—¦ Block oversized instances
    â—¦ Enforce tagging
â€¢ Regular cost reviews (FinOps culture)
ğŸ³ Containers & Kubernetes
8. A Kubernetes pod keeps restarting. How do you troubleshoot it?
ğŸ”¹ Step 1: Confirm Why the Pod Is Restarting

kubectl get pod <pod-name>

Check:
â€¢ STATUS â†’ CrashLoopBackOff, Error, OOMKilled
Then:

kubectl describe pod <pod-name>

Look at:
â€¢ Last State
â€¢ Exit codes
â€¢ Events (often the biggest clue)
ğŸ”¹ Step 2: Check Container Logs

kubectl logs <pod-name>

If it already restarted:

kubectl logs <pod-name> --previous

Look for:
â€¢ App crashes
â€¢ Stack traces
â€¢ Config or env variable errors
ğŸ”¹ Step 3: Identify Common Root Causes
1ï¸âƒ£ Application Crash
â€¢ Unhandled exception
â€¢ Misconfigured startup command
â€¢ Missing config or secrets
â¡ï¸ Fix app config or code.
2ï¸âƒ£ Liveness / Readiness Probe Failures
â€¢ Probe too aggressive
â€¢ App needs more startup time
Check:

livenessProbe:
readinessProbe:

Fix:
â€¢ Increase initialDelaySeconds
â€¢ Adjust timeoutSeconds / failureThreshold
3ï¸âƒ£ OOMKilled (Out of Memory)
â€¢ Pod exceeds memory limits
Check:

kubectl describe pod

Fix:
â€¢ Increase memory limits
â€¢ Optimize memory usage
â€¢ Add HPA if needed
4ï¸âƒ£ Image Pull Issues
â€¢ Wrong image tag
â€¢ Registry auth problems
Check events:

ImagePullBackOff

Fix:
â€¢ Correct image
â€¢ Fix imagePullSecrets
5ï¸âƒ£ Dependency Failures
â€¢ DB / API not reachable
â€¢ App exits when dependency is down
Fix:
â€¢ Add retries
â€¢ Use readiness probes instead of crashing
â€¢ Implement graceful startup logic
ğŸ”¹ Step 4: Validate Configuration
â€¢ ConfigMaps mounted?
â€¢ Secrets present?
â€¢ Correct env vars?

kubectl describe pod

ğŸ”¹ Step 5: Check Node & Resource Pressure
â€¢ Node out of memory or disk
â€¢ Pod evicted

kubectl describe node <node-name>

ğŸ”¹ Step 6: Debug Interactively (If Needed)

kubectl exec -it <pod-name> -- /bin/sh

Check:
â€¢ Files
â€¢ Env vars
â€¢ Network access
ğŸ”¹ Step 7: Fix & Roll Forward
â€¢ Apply fix
â€¢ Restart deployment:

kubectl rollout restart deployment <name>

Monitor:
â€¢ Pod stability
â€¢ Restart count
â€¢ Error rate
9. One microservice is consuming excessive CPU in the cluster. Whatâ€™s your approach?
ğŸ”¹ Phase 1: Confirm & Scope the Problem
1ï¸âƒ£ Identify the Offending Service
â€¢ Check metrics:
    â—¦ Prometheus / Grafana
    â—¦ kubectl top pods
â€¢ Verify:
    â—¦ Is CPU high for one pod or all replicas?
    â—¦ Is it sustained or spiky?Always confirm before acting.
ğŸ”¹ Phase 2: Contain the Impact
2ï¸âƒ£ Protect the Cluster
â€¢ Apply or verify:
    â—¦ CPU requests and limits
    â—¦ Pod disruption budgets
â€¢ Prevent noisy neighbor problemsOne bad service should never starve the cluster.
ğŸ”¹ Phase 3: Identify the Root Cause
3ï¸âƒ£ Check Traffic Patterns
â€¢ Has request volume increased?
â€¢ Any new consumers?
â€¢ Is it being hammered by retries or bots?
4ï¸âƒ£ Analyze Application Behavior
â€¢ Check:
    â—¦ Logs for tight loops
    â—¦ Inefficient algorithms
    â—¦ Serialization or parsing hot paths
â€¢ Profile if possible (pprof, JVM tools)
5ï¸âƒ£ Look for Dependency Issues
â€¢ Slow downstream service causing retries
â€¢ Missing timeouts or circuit breakersHigh CPU is often symptom, not cause.
ğŸ”¹ Phase 4: Fix or Mitigate
6ï¸âƒ£ Short-Term Mitigation
â€¢ Scale horizontally (HPA)
â€¢ Rate-limit incoming requests
â€¢ Enable caching
â€¢ Reduce log verbosity
7ï¸âƒ£ Long-Term Fix
â€¢ Optimize code paths
â€¢ Improve caching
â€¢ Add async processing
â€¢ Tune CPU requests/limits
â€¢ Add autoscaling based on:
    â—¦ CPU
    â—¦ RPS
    â—¦ Latency
ğŸ”¹ Phase 5: Prevent Recurrence
8ï¸âƒ£ Improve Observability
â€¢ Service-level dashboards
â€¢ CPU per request metrics
â€¢ Alerts on abnormal CPU growth
9ï¸âƒ£ Add Guardrails
â€¢ Resource quotas
â€¢ Admission controllers
â€¢ Cost and usage alerts
10. How would you deploy a zero-downtime update in Kubernetes?
ğŸ§  Core PrincipleNever take down all replicas at once, and never send traffic to an unready pod.
ğŸ”¹ Step 1: Use a Deployment (Not Naked Pods)
â€¢ Deploy apps using Deployment or StatefulSet
â€¢ Ensure:
    â—¦ At least 2 replicas
    â—¦ Pod is stateless
ğŸ”¹ Step 2: Configure Rolling Update Strategy

strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 0
    maxSurge: 1

â€¢ maxUnavailable: 0 â†’ no downtime
â€¢ maxSurge: 1 â†’ bring up new pod before killing old one
ğŸ”¹ Step 3: Proper Readiness Probes (Critical)

readinessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 10

â€¢ Traffic is sent only when the pod is ready
â€¢ Unready pods never receive trafficReadiness probes are the backbone of zero downtime.
ğŸ”¹ Step 4: Graceful Shutdown Handling
â€¢ Handle SIGTERM properly
â€¢ Add:

terminationGracePeriodSeconds: 30

â€¢ Let:
    â—¦ In-flight requests finish
    â—¦ Load balancer drain connections
ğŸ”¹ Step 5: Use Pod Disruption Budgets (PDBs)

minAvailable: 1

â€¢ Prevents voluntary disruptions from killing all pods
â€¢ Protects during node drains or cluster upgrades
ğŸ”¹ Step 6: Validate During Rollout

kubectl rollout status deployment <name>

Monitor:
â€¢ Error rates
â€¢ Latency
â€¢ Pod restarts
ğŸ”¹ Step 7: Rollback If Needed

kubectl rollout undo deployment <name>

Rollback should be:
â€¢ Fast
â€¢ Automatic
â€¢ Safe
ğŸ” Security & Compliance
11. Secrets are accidentally committed to Git. What immediate and long-term steps do you take?
ğŸš¨ Immediate Actions (First Minutes)
1ï¸âƒ£ Assume the Secret Is Compromised
â€¢ Treat it as already leaked
â€¢ Do not wait to see if it was abusedDeleting the commit alone is never enough.
2ï¸âƒ£ Revoke & Rotate the Secret Immediately
â€¢ Rotate:
    â—¦ API keys
    â—¦ DB passwords
    â—¦ Tokens
â€¢ Invalidate:
    â—¦ Sessions
    â—¦ Certificates (if applicable)
â€¢ Use:
    â—¦ AWS Secrets Manager / Vault / IAM
3ï¸âƒ£ Stop Further Exposure
â€¢ Remove secrets from:
    â—¦ Repo
    â—¦ CI logs
    â—¦ Artifacts
â€¢ Make repo private if needed
â€¢ Notify security team
ğŸ” Short-Term Cleanup
4ï¸âƒ£ Remove Secrets From Git History
â€¢ Use:
    â—¦ git filter-repo
    â—¦ BFG Repo-Cleaner
â€¢ Force-push cleaned history
â€¢ Invalidate forks if possible
âš ï¸ Still assume the secret was copied.
5ï¸âƒ£ Audit for Abuse
â€¢ Check:
    â—¦ CloudTrail
    â—¦ Access logs
    â—¦ DB logs
â€¢ Look for:
    â—¦ Unauthorized access
    â—¦ Suspicious API calls
ğŸ›¡ï¸ Long-Term Prevention
6ï¸âƒ£ Move Secrets Out of Code
â€¢ Use:
    â—¦ AWS Secrets Manager / Parameter Store
    â—¦ HashiCorp Vault
    â—¦ Kubernetes Secrets
â€¢ Inject secrets at runtime
7ï¸âƒ£ Add Guardrails
â€¢ Pre-commit hooks:
    â—¦ git-secrets
    â—¦ detect-secrets
â€¢ CI secret scanning:
    â—¦ GitHub secret scanning
    â—¦ TruffleHog
â€¢ Block commits with secrets automatically
8ï¸âƒ£ Improve Access Control
â€¢ Use:
    â—¦ IAM roles instead of static credentials
â€¢ Enforce:
    â—¦ Least privilege
    â—¦ Short-lived credentials
9ï¸âƒ£ Educate & Document
â€¢ Update:
    â—¦ Secure coding guidelines
    â—¦ Incident playbooks
â€¢ Run a blameless postmortem
12. How do you secure a CI/CD pipeline?
ğŸ§  Core PrincipleThe CI/CD pipeline is productionâ€™s front door â€” compromise it and everything downstream is compromised.
ğŸ”¹ 1. Secure Source Control
â€¢ Protect branches:
    â—¦ Require PR reviews
    â—¦ Enforce status checks
â€¢ Restrict who can:
    â—¦ Push to main
    â—¦ Modify pipeline configs
â€¢ Enable:
    â—¦ Signed commits
    â—¦ MFA for all users
ğŸ”¹ 2. Secrets Management (Critical)
â€¢ Never hardcode secrets
â€¢ Store secrets in:
    â—¦ AWS Secrets Manager / Vault
    â—¦ CI secret stores
â€¢ Use:
    â—¦ Short-lived credentials
    â—¦ IAM roles / OIDC (no static keys)
â€¢ Mask secrets in logs
ğŸ”¹ 3. Secure the CI Runners
â€¢ Use:
    â—¦ Ephemeral runners (preferred)
    â—¦ Isolated build environments
â€¢ Restrict:
    â—¦ Network access
    â—¦ Privileged containers
â€¢ Patch runners regularly
ğŸ”¹ 4. Dependency & Code Security (Shift Left)
â€¢ SAST (static analysis)
â€¢ Dependency scanning (CVEs)
â€¢ Secrets scanning
â€¢ Fail builds on high-severity issues
ğŸ”¹ 5. Secure Build Artifacts
â€¢ Build once, deploy many times
â€¢ Sign artifacts:
    â—¦ Container image signing (cosign)
â€¢ Store in private registries
â€¢ Prevent artifact overwrites
ğŸ”¹ 6. Deployment Security
â€¢ Use:
    â—¦ Least-privilege service accounts
    â—¦ Environment-specific roles
â€¢ Require:
    â—¦ Manual approvals for prod
â€¢ Separate:
    â—¦ Build vs deploy permissions
ğŸ”¹ 7. Infrastructure as Code Security
â€¢ Scan IaC:
    â—¦ Terraform / CloudFormation checks
â€¢ Enforce policies:
    â—¦ OPA / Sentinel
â€¢ Prevent insecure infra from being deployed
ğŸ”¹ 8. Observability & Auditing
â€¢ Log:
    â—¦ Pipeline executions
    â—¦ Access changes
â€¢ Enable:
    â—¦ CloudTrail / audit logs
â€¢ Monitor:
    â—¦ Unusual pipeline behavior
ğŸ”¹ 9. Protect the Supply Chain
â€¢ Pin dependencies
â€¢ Verify checksums
â€¢ Use trusted base images
â€¢ Prevent dependency confusion attacks.
13. A vulnerability is found in a production container image. What do you do?
ğŸ”¹ Step 1: Assess the Risk Immediately
â€¢ Check:
    â—¦ Severity of the vulnerability (CVSS score, exploitability)
    â—¦ Whether it affects your application functionality
â€¢ Identify:
    â—¦ Which environments and containers are affected
    â—¦ Whether the container is exposed to the publicAssume the worst-case scenario until confirmed otherwise.
ğŸ”¹ Step 2: Mitigate Exposure Quickly
â€¢ If critical:
    â—¦ Remove the vulnerable container from public access or routing
    â—¦ Disable affected services temporarily if needed
â€¢ For less critical:
    â—¦ Monitor closely
    â—¦ Apply temporary firewall, WAF, or network isolation
ğŸ”¹ Step 3: Patch & Rebuild
â€¢ Pull the latest base image with the security fix
â€¢ Rebuild the container from scratch
â€¢ Run all CI/CD tests to ensure the patch does not break functionality
â€¢ Push the new image to your private registry
ğŸ”¹ Step 4: Redeploy Safely
â€¢ Use zero-downtime deployment strategies:
    â—¦ Rolling update
    â—¦ Blue-Green
    â—¦ Canary
â€¢ Ensure:
    â—¦ Health checks pass
    â—¦ Monitoring dashboards show stable performance
ğŸ”¹ Step 5: Remove Vulnerable Images
â€¢ Delete the old image from:
    â—¦ Registries
    â—¦ Build caches
â€¢ Prevent accidental redeployments
ğŸ”¹ Step 6: Verify & Monitor
â€¢ Monitor:
    â—¦ Error rates
    â—¦ Security alerts
    â—¦ Traffic anomalies
â€¢ Run vulnerability scanning on deployed images to confirm the fix
ğŸ”¹ Step 7: Implement Long-Term Prevention
â€¢ Enable image scanning in CI/CD (Snyk, Trivy, Clair, Anchore)
â€¢ Pin base images to known versions
â€¢ Automate rebuilds on base image updates
â€¢ Adopt a supply chain security approach (SLSA / SBOM)
ğŸ“Š Monitoring, Logging & Incident Response
14. Users report intermittent latency, but monitoring shows everything â€œgreen.â€ What next?
ğŸ”¹ Step 1: Confirm the Issue
â€¢ Collect as much detail from users as possible:
    â—¦ Time of latency spikes
    â—¦ Which endpoints / services are affected
    â—¦ Geography / device patterns
â€¢ Try to reproduce the latency yourselfSometimes usersâ€™ perception differs from system dashboards.
ğŸ”¹ Step 2: Check Detailed Metrics
â€¢ â€œGreenâ€ in high-level dashboards may hide spikes. Check:
    â—¦ Percentiles (p95/p99), not just averages
    â—¦ Latency by endpoint / service / pod / node
    â—¦ Traffic volume / request bursts
    â—¦ Database query times and queue lengthsHigh tail latency often hides in averages.
ğŸ”¹ Step 3: Inspect Logs & Traces
â€¢ Enable distributed tracing (X-Ray, Jaeger, OpenTelemetry)
â€¢ Look for:
    â—¦ Slow requests
    â—¦ Timeouts waiting on dependencies
    â—¦ Retries or failed connections
â€¢ Check application logs for errors or warnings not captured in metrics
ğŸ”¹ Step 4: Check External Dependencies
â€¢ DB, cache, third-party APIs, or message queues
â€¢ Look for intermittent slow queries or throttling
â€¢ Sometimes the app itself is fine; latency is upstream
ğŸ”¹ Step 5: Network & Infrastructure
â€¢ Inspect:
    â—¦ VPC / subnet / firewall issues
    â—¦ Load balancer spikes / connection limits
    â—¦ DNS resolution latency
    â—¦ Pod-to-pod network latency in KubernetesNetwork blips often cause intermittent issues while metrics look green on average.
ğŸ”¹ Step 6: Resource Contention
â€¢ Check node-level CPU/memory spikes or throttling
â€¢ Check JVM / GC pauses if running Java apps
â€¢ Container CPU/memory limits causing throttling
ğŸ”¹ Step 7: Deploy Instrumentation (If Needed)
â€¢ Add more granular metrics:
    â—¦ Endpoint-level timing
    â—¦ DB query duration histograms
    â—¦ Request queue depth
â€¢ This helps capture tail latency spikes
ğŸ”¹ Step 8: Communicate & Mitigate
â€¢ Inform users / stakeholders about investigation
â€¢ If possible, implement temporary mitigation:
    â—¦ Enable caching
    â—¦ Reduce non-critical features
    â—¦ Retry/backoff optimization
15. A critical service goes down at 2 AM. Walk me through your response.
ğŸ§° Phase 1: Immediate Response (First 0â€“5 minutes)
1ï¸âƒ£ Acknowledge & Triage
â€¢ Respond immediately to the alert (PagerDuty / Opsgenie / Slack)
â€¢ Identify:
    â—¦ Which service is down
    â—¦ Impact scope (number of users, criticality)
â€¢ Communicate:â€œIncident acknowledged â€” investigating.â€
2ï¸âƒ£ Mitigate Customer Impact
â€¢ Route traffic away if possible:
    â—¦ Use load balancer health checks
    â—¦ Disable affected endpoints
â€¢ Enable fallbacks or feature flags
â€¢ Goal: minimize user impact while investigation proceeds
ğŸ§ª Phase 2: Investigate the Root Cause (5â€“20 minutes)
3ï¸âƒ£ Check System & Service Health
â€¢ kubectl get pods / kubectl describe for K8s
â€¢ Check auto-scaling, node availability
â€¢ Verify load balancers, DNS, network connectivity
4ï¸âƒ£ Examine Logs & Metrics
â€¢ CloudWatch / Prometheus / Grafana dashboards
â€¢ Application logs for errors
â€¢ Metrics for CPU, memory, disk, request latency, queue depth
5ï¸âƒ£ Identify Recent Changes
â€¢ Deployment? Config change? Infra update?
â€¢ Rollback candidate if new release caused failure
âš¡ Phase 3: Fix or Mitigate (20â€“60 minutes)
6ï¸âƒ£ Quick Mitigation
â€¢ Redeploy last stable version
â€¢ Restart failing pods / services
â€¢ Remove misbehaving nodes or containers
â€¢ Activate backup / failover systems if available
7ï¸âƒ£ Confirm Recovery
â€¢ Ensure the service is healthy (kubectl rollout status, metrics normalized)
â€¢ Verify users can access functionality
ğŸ“ Phase 4: Post-Incident Actions
8ï¸âƒ£ Root Cause Analysis (RCA)
â€¢ Determine underlying cause:
    â—¦ Bug, configuration error, infrastructure failure, external dependency
â€¢ Document timeline and decisions
â€¢ Include metrics and logs for reference
9ï¸âƒ£ Prevent Recurrence
â€¢ Adjust monitoring & alerts (p95 latency, error spikes)
â€¢ Fix deployment pipeline or IaC issues
â€¢ Update runbooks
16. How do you decide what metrics to monitor for a new service?
ğŸ§  Core PrincipleMonitor what matters to reliability, performance, and business outcomes.
ğŸ”¹ Step 1: Understand the Service
â€¢ Ask:
    â—¦ What does the service do? (business context)
    â—¦ What are critical paths / endpoints?
    â—¦ Which dependencies matter (DB, cache, external APIs)?Example: A payment service â†’ success rate, latency, transaction volume.
ğŸ”¹ Step 2: Monitor Infrastructure Health
â€¢ CPU / memory / disk / network of nodes or containers
â€¢ Pod / instance status (Kubernetes pod restarts, node health)
â€¢ Autoscaling metrics (desired vs actual replicas)Ensures the service has enough resources.
ğŸ”¹ Step 3: Monitor Application Health
â€¢ Request rate / throughput
â€¢ Latency (p50, p95, p99)
â€¢ Error rates (4xx / 5xx, exceptions, failed jobs)
â€¢ Queue depth (for async processing)Captures internal performance and reliability.
ğŸ”¹ Step 4: Monitor External Dependencies
â€¢ Database:
    â—¦ Connection pool usage, query latency, replication lag
â€¢ Caches:
    â—¦ Hit ratio, eviction rate
â€¢ External APIs:
    â—¦ Success rate, latency, rate limitsSlow dependencies often cause service degradation.
ğŸ”¹ Step 5: Monitor Business Metrics (Optional but Valuable)
â€¢ Number of transactions per minute
â€¢ Orders processed
â€¢ Signups / logins / feature usageDetect issues invisible in system metrics.
ğŸ”¹ Step 6: Define Alerts Carefully
â€¢ Avoid alert fatigue
â€¢ Alert thresholds for:
    â—¦ High error rate
    â—¦ Latency spikes
    â—¦ Resource exhaustion
â€¢ Use multi-metric correlation for smarter alerts
ğŸ”¹ Step 7: Review and Iterate
â€¢ After initial deployment:
    â—¦ Add/remove metrics based on real-world behavior
    â—¦ Monitor tail latency, rare errors, saturation points
â€¢ Continuous improvement â†’ dashboards evolve with service
ğŸ“¦ Infrastructure as Code (IaC)
17. A Terraform apply fails in production. Whatâ€™s your rollback plan?
ğŸ§  Core PrincipleAlways assume Terraform can fail. Have a rollback plan and minimize blast radius.
ğŸ”¹ Step 1: Pause & Assess
â€¢ Do not panic â€” stop further changes.
â€¢ Check:
    â—¦ Error message (terraform apply output)
    â—¦ Which resources failed / partially applied
    â—¦ State file status (local or remote backend)Understanding what went wrong before acting is critical.
ğŸ”¹ Step 2: Identify the Impacted Resources
â€¢ Use:

terraform plan
terraform state list

â€¢ Determine:
    â—¦ Resources fully created
    â—¦ Resources partially modified
    â—¦ Dependencies that could breakPartial applies are the biggest risk in production.
ğŸ”¹ Step 3: Rollback Options
Option A: Terraform Rollback
â€¢ Terraform doesnâ€™t have a native â€œundoâ€, but you can:
    1. Restore previous state file from remote backend (S3 / Terraform Cloud)
    2. Run:

terraform apply

â†’ This will revert infrastructure to the last known good state
Option B: Manual Rollback
â€¢ For critical resources (databases, networking):
    â—¦ Revert configuration manually
    â—¦ Use snapshots/backups if needed
    â—¦ Apply changes incrementally
Option C: Blue-Green / Immutable Infrastructure
â€¢ If using:
    â—¦ EC2 / ECS / Kubernetes / Terraform modules
â€¢ Switch traffic back to previous environment
â€¢ Destroy failed resources after verification
ğŸ”¹ Step 4: Verify Recovery
â€¢ Validate:
    â—¦ Services are up and healthy
    â—¦ Configurations match desired state
    â—¦ No downtime for users (if applicable)
ğŸ”¹ Step 5: Post-Incident
â€¢ Analyze:
    â—¦ Why terraform apply failed (syntax, permissions, resource limits, API errors)
    â—¦ Why rollback was needed
â€¢ Prevent recurrence:
    â—¦ Use Terraform plan + approval in production
    â—¦ Enable state versioning / locking
    â—¦ Use smoke tests before applying critical changes
18. Multiple teams modify the same infrastructure. How do you prevent conflicts?
ğŸ§  Core PrincipleInfrastructure should be versioned, collaborative, and conflict-free â€” like code.
ğŸ”¹ Step 1: Use Remote State with Locking
â€¢ Store state in remote backends:
    â—¦ S3 + DynamoDB (locking)
    â—¦ Terraform Cloud / Enterprise
â€¢ Enable state locking to prevent multiple applies at the same timePrevents concurrent modifications that break the state.
ğŸ”¹ Step 2: Modularize Infrastructure
â€¢ Split infra into independent modules:
    â—¦ Networking, DB, compute, monitoring
â€¢ Assign ownership per module to different teams
â€¢ Avoid multiple teams modifying the same module simultaneouslySmaller modules = smaller blast radius.
ğŸ”¹ Step 3: Version Control & Branching
â€¢ All changes go through Git:
    â—¦ Feature branches
    â—¦ Pull requests / merge requests
â€¢ Require:
    â—¦ Code review
    â—¦ Terraform plan review before merge
â€¢ CI validates plan against remote stateCode + plan review catches conflicts before apply.
ğŸ”¹ Step 4: Apply Approval Workflows
â€¢ Use manual approvals for production:
    â—¦ Terraform Cloud / Enterprise workspaces
    â—¦ GitHub Actions + protected branches
â€¢ Prevent accidental direct terraform apply
ğŸ”¹ Step 5: Use Workspaces or Environments
â€¢ Separate workspaces for:
    â—¦ Dev / QA / Staging / Prod
â€¢ Teams can test changes without affecting others
ğŸ”¹ Step 6: Naming Conventions & Scoping
â€¢ Consistent resource naming
â€¢ Avoid hardcoded values
â€¢ Scope resources by team or project to prevent accidental overlap
ğŸ”¹ Step 7: Monitor & Audit Changes
â€¢ Enable logging and audit trails:
    â—¦ Who applied what and when
    â—¦ State changes over time
â€¢ Helps quickly resolve conflicts if they occur
ğŸ¤ Collaboration & Process
19. Developers want faster releases, ops wants stability. How do you balance both?
ğŸ§  Core PrincipleBalance speed and stability by automating safe releases and building confidence in the system.
ğŸ”¹ Step 1: Implement CI/CD Pipelines
â€¢ Developers push code frequently
â€¢ Pipelines automatically:
    â—¦ Build
    â—¦ Run unit tests
    â—¦ Run integration tests
â€¢ Artifacts are immutable and ready for deploymentFast feedback catches issues before reaching production.
ğŸ”¹ Step 2: Use Progressive Deployment Strategies
â€¢ Canary deployments: release to a small subset of users first
â€¢ Blue-Green deployments: switch traffic only when the new version is ready
â€¢ Feature flags: release features safely without full rolloutReduces risk while allowing faster releases.
ğŸ”¹ Step 3: Automate Testing and Validation
â€¢ Automated tests:
    â—¦ Unit, integration, contract, performance
â€¢ Pre-prod environments:
    â—¦ Mirror production for realistic testing
â€¢ Static analysis / security scanning
â€¢ Ensures stability doesnâ€™t block speed
ğŸ”¹ Step 4: Monitor & Rollback Quickly
â€¢ Observability in production:
    â—¦ Prometheus/Grafana metrics
    â—¦ Logs / distributed tracing
â€¢ Set up alerts and automated rollback triggers
â€¢ Confidence that if something goes wrong, it can be reverted immediately
ğŸ”¹ Step 5: Collaboration & Culture
â€¢ Shared metrics for both teams:
    â—¦ Deployment frequency
    â—¦ Change failure rate
    â—¦ Mean time to recovery (MTTR)
â€¢ Ops provides guardrails; developers innovate within safe boundariesTransparency and feedback loops align goals.
ğŸ”¹ Step 6: Small, Incremental Changes
â€¢ Smaller, frequent releases are safer than large, infrequent ones
â€¢ Easier to debug, easier to rollback
â€¢ Keeps velocity high without compromising stability
20. How do you introduce DevOps practices in a legacy organization?
ğŸ§  Core PrincipleDevOps is as much cultural as it is technical â€” you canâ€™t just install tools.
ğŸ”¹ Step 1: Assess the Current State
â€¢ Understand:
    â—¦ How teams build, test, deploy software
    â—¦ Pain points in development, operations, and QA
    â—¦ Existing tooling and workflows
â€¢ Identify low-hanging fruit for early winsBaseline knowledge helps prioritize initiatives and get buy-in.
ğŸ”¹ Step 2: Start with Culture & Collaboration
â€¢ Break down silos:
    â—¦ Devs, Ops, QA working together
â€¢ Introduce shared responsibility for:
    â—¦ Deployment success
    â—¦ Application reliability
â€¢ Encourage blameless postmortemsCulture is the hardest but most important part.
ğŸ”¹ Step 3: Automate the Basics
â€¢ Implement CI pipelines first:
    â—¦ Version control, automated builds, unit tests
â€¢ Gradually add:
    â—¦ Integration tests
    â—¦ Artifact repositories
    â—¦ Automated deployments to stagingEarly automation builds confidence and reduces friction.
ğŸ”¹ Step 4: Introduce Incremental Deployment Practices
â€¢ Start small:
    â—¦ Feature flags
    â—¦ Canary or blue-green deployments
â€¢ Target non-critical services first
â€¢ Measure impact and reliability before scaling to production
ğŸ”¹ Step 5: Monitoring & Observability
â€¢ Implement metrics, logging, and alerts for early wins:
    â—¦ Prometheus, Grafana, ELK stack, CloudWatch, etc.
â€¢ Teams can see the impact of changes and identify bottlenecksVisibility drives trust in DevOps processes.
ğŸ”¹ Step 6: Measure Success & Iterate
â€¢ Track KPIs:
    â—¦ Deployment frequency
    â—¦ Change failure rate
    â—¦ MTTR (mean time to recovery)
â€¢ Celebrate wins publicly
â€¢ Iterate on pipelines, automation, and processes
ğŸ”¹ Step 7: Expand Gradually
â€¢ After early wins:
    â—¦ Introduce Infrastructure as Code (Terraform, Ansible)
    â—¦ Implement secrets management
    â—¦ Apply security scanning / DevSecOps practicesDonâ€™t try to change everything at once â€” DevOps is evolutionary, not revolutionary.
